{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaa69f929be47d6ba83b82c92ba86c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8e806466ba45f78a48792bd64572fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bae70fb4d144fe592dca4bd9bf4341f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depends on g2p-en\n",
    "from datasets import load_dataset\n",
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "generics_kb = load_dataset(\n",
    "    'community-datasets/generics_kb', name='generics_kb_best', split='train')\n",
    "ponyspeech_dataset = load_dataset(\n",
    "    'synthbot/pony-speech', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAH1T WAH1N AA1N WAH1N, LEH1TS KLIY1N IH1T.\n"
     ]
    }
   ],
   "source": [
    "horsewords = {}\n",
    "with open('horsewords.clean') as f:\n",
    "    while line := f.readline():\n",
    "        spl = line.split('  ')\n",
    "        horsewords[spl[0]] = spl[1]\n",
    "\n",
    "def phonemize(text):\n",
    "    spl = text.split()\n",
    "    l = ''\n",
    "    for s in spl:\n",
    "        p = [arp for arp in g2p(s) if arp != ' ']\n",
    "        arpabet_string = ''.join(p)\n",
    "        l += arpabet_string + ' '\n",
    "    return l.strip()\n",
    "print(phonemize('But one on one, let\\'s clean it.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ponyspeech: 100%|██████████| 64783/64783 [04:42<00:00, 229.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "n = 240000\n",
    "corpus_output_txt = 'g2pen_corpus_ponyonly.txt'\n",
    "with open(corpus_output_txt, 'w', encoding='utf-8') as f:\n",
    "    #for text in tqdm(generics_kb.shuffle().select(\n",
    "    #    range(n)\n",
    "    #)['generic_sentence'], desc='generic_kb wiki'):\n",
    "    #    f.write(phonemize(text)+'\\n')\n",
    "    for text in tqdm(ponyspeech_dataset['transcription'], desc='ponyspeech'):\n",
    "        f.write(phonemize(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers import (decoders, models, normalizers,\n",
    "    pre_tokenizers, processors, trainers, Tokenizer)\n",
    "\n",
    "import re\n",
    "\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "    pre_tokenizers.Metaspace('▁'),\n",
    "    pre_tokenizers.Punctuation()\n",
    "    ])\n",
    "trainer = trainers.BpeTrainer(vocab_size=384,\n",
    "    special_tokens=special_tokens, unk_token='[UNK]')\n",
    "tokenizer.train(['g2pen_corpus_ponyonly.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "parler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁AY1', '▁NIY1D', '▁SAH1M', '▁M', 'AH1', 'NIY0', ',', '▁KAE1N', '▁AY1', '▁GEH1T', '▁SAH1M', '▁M', 'AH1', 'NIY0', '?', '▁AY1', '▁NIY1D', '▁WAH1N', '▁M', 'IH1L', 'Y', 'AH0N', '.']\n",
      "23\n",
      "False\n",
      "['▁I', '▁need', '▁some', '▁money', ',', '▁can', '▁I', '▁get', '▁some', '▁money', '?', '▁I', '▁need', '▁one', '▁million', '.']\n",
      "17\n",
      "['▁R', 'EY1', 'Z', '▁AH1P', ',', '▁GEH1T', '▁Y', 'ER0', 'S', 'EH1L', 'F', '▁T', 'AH0', 'G', 'EH1D', 'HER0', ',', '▁AH0ND', '▁DR', 'AY1', 'V', '▁DHAE1T', '▁F', 'AH1', 'NG', 'KIY0', '▁SOW1', 'L', '.']\n",
      "29\n",
      "False\n",
      "['▁R', 'aise', '▁up', ',', '▁get', '▁yourself', '▁together', ',', '▁and', '▁drive', '▁that', '▁fun', 'ky', '▁soul', '.']\n",
      "16\n",
      "['▁AW1ER0', '▁RIH0', 'Z', 'AH1', 'L', 'T', 'S', '▁D', 'EH1M', 'AH0N', 'ST', 'R', 'EY2', 'T', '▁HH', 'AY1', '▁F', 'AH0D', 'EH1L', 'AH0TIY0', '▁SP', 'IY1', 'CH', '▁JH', 'EH2', 'N', 'ER0', 'EY1', 'SHAH0N', '▁AH0ND', '▁AH0', '▁D', 'A', 'Y0', 'V', 'ER1', 'S', '▁REY1N', 'JH', '▁AH1V', '▁AE1', 'KS', 'EH0', 'NT', 'S']\n",
      "45\n",
      "False\n",
      "['▁Our', '▁results', '▁demonstrate', '▁high', '▁', 'fidelity', '▁speech', '▁generation', '▁and', '▁', 'a', '▁diverse', '▁range', '▁of', '▁accent', 's']\n",
      "17\n",
      "['▁IH0', 'K', 'W', 'EH1ST', 'RIY0', 'AH0', '▁DHAH0', '▁BR', 'EY1', 'V', '▁IH0', 'K', 'W', 'EH1ST', 'RIY0', 'AH0N', '.']\n",
      "17\n",
      "False\n",
      "['▁E', 'que', 'stria', '▁the', '▁brave', '▁E', 'quest', 'rian', '.']\n",
      "10\n",
      "['▁D', 'EH1D', '▁N', 'IH1R', 'IH0K', '▁ST', 'AO1R', 'AH0', 'JH', '.']\n",
      "10\n",
      "False\n",
      "['▁Dead', '▁', 'n', 'i', 'rik', '▁storage', '.']\n",
      "8\n",
      "['▁WEH1L', ',', '▁WAH1N', '▁AA1N', '▁WAH1N', ',', '▁LEH1T', 'S', '▁KL', 'IY1', 'N', '▁IH1T', '!']\n",
      "13\n",
      "False\n",
      "['▁Well', ',', '▁one', '▁on', '▁one', ',', '▁let', \"'\", 's', '▁clean', '▁it', '!']\n",
      "13\n",
      "['▁AE1', 'M', 'AH0Z', 'AA2', 'N', '▁DIH0', 'L', 'IH1', 'VER0', 'Z', '▁P', 'AE1K', 'AH0', 'JH', 'AH0Z', '▁K', 'W', 'IH1K', 'LIY0', '▁AH0', 'K', 'R', 'AO1', 'S', '▁DHAH0', '▁Y', 'UW', '0', 'N', 'AY1T', 'AH0D', '▁ST', 'EY1T', 'S', '!']\n",
      "35\n",
      "False\n",
      "['▁Amazon', '▁delivers', '▁packages', '▁quickly', '▁across', '▁the', '▁United', '▁States', '!']\n",
      "10\n",
      "['▁B', 'IH1T', 'K', 'AH0N', '▁AH0ND', '▁', 'EH1T', 'H', 'RIY0', 'AH0M', '▁AA1R', '▁P', 'AA1P', 'Y', 'AH0L', 'ER0', '▁KR', 'IH1', 'P', 'K', 'AH0', 'Y', 'ER', '2', 'AH0S', 'NIY0', 'Z', '.']\n",
      "28\n",
      "False\n",
      "['▁Bitcoin', '▁and', '▁Ethereum', '▁are', '▁popular', '▁', 'cryptocurrencies', '.']\n",
      "9\n",
      "['▁T', 'EH1K', 'ST', 'S', 'P', 'EH2', 'K', '▁M', 'AA1', 'D', 'AH0L', 'Z', '▁WAH1N', '▁TUW1', 'HH', 'AH1', 'ND', 'R', 'AH0D', 'D', 'AA1', 'L', 'ER0Z']\n",
      "23\n",
      "False\n",
      "['▁Text', '-', 'to', '-', 's', 'pe', 'ech', '▁models', '▁1', '▁$200']\n",
      "11\n",
      "['▁K', 'YUW1', '▁K', 'YUW1', 'IH0NG']\n",
      "5\n",
      "False\n",
      "['▁Que', 'u', 'e', '▁que', 'u', 'ing']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing'\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(encoding.tokens)\n",
    "    print(len(encoding.tokens))\n",
    "    print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    print(parler_tokenizer.tokenize(test_text))\n",
    "    print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁DHAH0', '▁F', 'AH0', 'TH', 'IH1S', 'IH0K', '▁S', 'F', 'IH1NG', 'KS', '▁ST', 'EH1L', 'TH', 'IY0', 'LIY0', '▁', 'EH0', 'S', 'CH', 'UW1', 'Z', '▁DHAH0', '▁N', 'IH0', 'M', 'AA1', 'N', 'IH0K', '▁N', 'UW', '0', 'M', 'AE1', 'T', 'IH0K', '▁N', 'IH1K', 'N', 'AE', '2', 'KS', '.']\n",
      "42\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'this', 'ic', '▁', 's', 'phin', 'x', '▁steal', 'thi', 'ly', '▁', 'e', 'sche', 'w', 's', '▁the', '▁', 'm', 'nem', 'onic', '▁pneu', 'matic', '▁', 'k', 'nick', 'k', 'n', 'ack', 's', '.']\n",
      "34\n",
      "['▁SHIY1', 'S', 'IH0N', 'Z', '▁SK', 'W', 'ER1', 'AH0L', '▁CH', 'ER1', 'P', 'T', '▁R', 'IH1D', 'H', 'M', 'IH0K', 'LIY0', '▁WAY1', 'L', '▁M', 'AH1N', 'CH', 'IH0NG', '▁AA1N', '▁', 'Z', 'W', 'IH2', 'T', 'ER0', 'SH', 'IY1', 'N', 'IH0NG', '▁K', 'W', 'EH1T', 'Z', 'AH0L', '▁K', 'W', 'IH1L', 'Z', '.']\n",
      "45\n",
      "False\n",
      "['▁S', 'z', 'c', 'zeci', 'n', \"'\", 's', '▁squirrel', '▁', 'chir', 'ped', '▁rhythm', 'ically', '▁while', '▁mun', 'ching', '▁on', '▁', 'z', 'wit', 'terio', 'nic', '▁qu', 'etz', 'al', '▁qui', 'll', 's', '.']\n",
      "30\n",
      "['▁B', 'AO1R', 'N', 'Z', '▁F', 'ER0', 'G', 'EH1D', 'L', 'HH', 'IH0N', '▁L', 'IH1NG', 'KS', '▁F', 'EH1T', 'H', 'AH0L', 'EY2', 'T', '▁S', 'AY1K', 'IY0', '▁WAA1Z', '▁K', 'W', 'AY1T', '▁AH0', '▁K', 'AH0N', 'AH1', 'ND', 'R', 'AH0M', '.']\n",
      "35\n",
      "False\n",
      "['▁B', 'jor', 'n', \"'\", 's', '▁', 'f', 'jor', 'd', '-', 'd', 'well', 'ing', '▁', 'ly', 'n', 'x', '▁', 'phthal', 'ate', '▁', 'psych', 'e', '▁was', '▁quite', '▁', 'a', '▁con', 'und', 'rum', '.']\n",
      "32\n",
      "['▁DHAH0', '▁T', 'S', 'AY1T', 'G', 'AY2', 'ST', '▁AH1V', '▁JH', 'AO1R', 'G', 'IY0', 'IH0', 'KS', 'IY0', 'Z', '▁K', 'AA1', 'S', 'R', 'AA', '0', 'D', '▁', 'Z', 'AY1', 'L', 'AH0', 'F', 'OW2', 'N', '▁K', 'W', 'IH0', 'NT', 'EH1T', '▁WAA1Z', '▁TR', 'UW1', 'LIY0', '▁Y', 'UW', '0', 'N', 'IY1', 'K', '.']\n",
      "47\n",
      "False\n",
      "['▁The', '▁', 'zeit', 'ge', 'ist', '▁of', '▁Gy', 'ör', 'g', 'y', 'ike', \"'\", 's', '▁', 'c', 's', 'á', 'r', 'd', 'á', 's', '▁', 'x', 'y', 'l', 'o', 'phone', '▁qu', 'inte', 't', '▁was', '▁truly', '▁unique', '.']\n",
      "35\n",
      "['▁', 'Z', 'AA1R', '▁', 'Z', 'AY1', 'L', 'AH0', 'F', 'OW2', 'N', 'Z', '▁CH', 'ER0', 'IY1', 'NAH0', '▁W', 'IH1S', 'P', 'ER0', 'D', '▁', 'Z', 'UW', '2', 'Z', 'AH0', 'G', 'L', 'IY1', 'K', 'AH0', '▁WAY1', 'L', '▁', 'IY1T', 'IH0NG', '▁S', 'AW1', 'ER0', 'K', 'R', 'A', 'W', '2', 'T', '▁IH0N', '▁AA1R', 'HH', 'AH0S', '.']\n",
      "51\n",
      "False\n",
      "['▁T', 's', 'ar', '▁', 'X', 'y', 'l', 'o', 'phone', \"'\", 's', '▁', 'c', 'zar', 'in', 'a', '▁whisper', 'e', 'd', '▁', 'ž', 'u', 'ž', 'e', 'l', 'j', 'ka', '▁while', '▁eating', '▁sau', 'er', 'kraut', '▁in', '▁', 'Å', 'r', 'hus', '.']\n",
      "39\n",
      "['▁K', 'YUW1', 'IH0NG', '▁FAO1R', '▁L', 'AE', '2', 'N', 'F', 'OW0', 'D', 'ER0', 'K', 'OW0', 'L', 'Y', 'AE1', 'F', 'IH0K', 'ST', '▁A', 'A', '2', 'N', 'AH0M', 'OW0', 'T', 'AH0', 'P', 'EY1', 'SH', 'IY0', '▁AY1', 'ST', 'IH0', 'D', 'F', 'AO', '0', 'LD', '.']\n",
      "41\n",
      "False\n",
      "['▁Que', 'u', 'ing', '▁for', '▁L', 'lan', 'fair', 'p', 'w', 'll', 'g', 'wyn', 'g', 'y', 'll', 'go', 'ger', 'ych', 'w', 'y', 'r', 'ndro', 'b', 'w', 'll', 'll', 'ant', 'y', 'sili', 'o', 'go', 'go', 'go', 'ch', \"'\", 's', '▁on', 'omato', 'p', 'o', 'e', 'ic', '▁', 'e', 'i', 'sted', 'd', 'f', 'o', 'd', '.']\n",
      "52\n",
      "['▁DHAH0', '▁KL', 'EH2', 'P', 'T', 'AH0M', 'AA1', 'NAH0', 'KIY0', 'Z', '▁SH', 'W', 'AH0N', 'AA1', 'M', 'AH0', '▁D', 'AY2', 'AH0', 'G', 'N', 'OW1', 'S', 'AH0S', '▁L', 'EH1', 'FT', '▁DHAH0', '▁S', 'AH0', 'K', 'AY1', 'AH0T', 'R', 'AH0ST', '▁N', 'AA', '0', 'N', 'P', 'L', 'AH1ST', '.']\n",
      "43\n",
      "False\n",
      "['▁The', '▁', 'kle', 'p', 'to', 'mania', 'c', \"'\", 's', '▁', 'schw', 'an', 'nom', 'a', '▁diagnosis', '▁left', '▁the', '▁psychiatrist', '▁non', 'plus', 'sed', '.']\n",
      "23\n",
      "['▁', 'Z', 'ER1', 'KS', 'IY0', 'Z', \"'\", '▁', 'Z', 'IH0', 'KS', 'W', 'AH0', 'F', 'S', 'AO1R', 'AH0', '▁IH0', 'G', 'Z', 'IH1', 'B', 'AH0T', 'AH0D', '▁', 'Z', 'EH2', 'NAH0', 'F', 'AA1', 'B', 'IH0K', '▁BIH0', 'HH', 'EY1', 'V', 'Y', 'ER0', '▁T', 'AH0', 'W', 'AO1R', 'D', 'Z', '▁', 'Z', 'H', 'AA1', '▁', 'Z', 'AA1', 'S', '▁', 'Z', 'H', 'IH0', 'V', 'AA1', 'G', 'OW0', '▁', 'Z', 'UW1', 'HH', 'IH0', '.']\n",
      "66\n",
      "False\n",
      "['▁', 'X', 'er', 'x', 'e', 's', \"'\", '▁', 'x', 'i', 'pho', 's', 'ura', '▁', 'exhibited', '▁', 'x', 'en', 'o', 'pho', 'bic', '▁behavior', '▁towards', '▁Z', 's', 'a', '▁Z', 's', 'a', \"'\", 's', '▁Z', 'h', 'iva', 'go', '▁', 'z', 'h', 'u', 'z', 'h', '.']\n",
      "43\n",
      "['▁DHAH0', '▁FL', 'EH0', 'G', 'M', 'AE1', 'T', 'IH0K', '▁F', 'AA1R', 'M', 'AH0S', 'IH0', 'ST', 'S', '▁', 'EH2', 'M', 'F', 'AH0Z', 'IY1', 'M', 'AH0', '▁WAA1Z', '▁IH0', 'G', 'Z', 'AE1', 'S', 'ER0', 'B', 'EY2', 'T', 'AH0D', '▁B', 'AY1', '▁F', 'EH1', 'NT', 'H', 'AH0S', \"'\", '▁TH', 'AH0N', 'OW1', 'IH0K', '▁KR', 'IH0', 'S', 'AE1', 'NT', 'H', 'AH0M', 'AH0M', 'Z', '.']\n",
      "56\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'leg', 'matic', '▁pharmacist', \"'\", 's', '▁', 'e', 'm', 'phy', 's', 'e', 'm', 'a', '▁was', '▁', 'exacerbate', 'd', '▁by', '▁Ph', 'th', 'on', 'us', \"'\", '▁', 'ch', 'th', 'onic', '▁', 'ch', 'ry', 's', 'ant', 'hem', 'um', 's', '.']\n",
      "41\n",
      "['▁K', 'W', 'AH0', 'JH', 'IY1', 'B', 'OW0', 'Z', '▁S', 'IH1', 'Z', 'JH', 'IY0', '▁WIH1DH', '▁P', 'ER0Z', 'B', 'IH1G', 'Z', 'F', 'IY0', 'Z', '▁S', 'AY1K', 'IY0', '▁K', 'AA1', 'Z', 'D', '▁AH0', '▁HH', 'AH1', 'B', 'AH0', 'B', '▁IH0N', '▁W', 'AA2', 'G', 'AH0D', 'UW1', 'G', 'UW', '0', '.']\n",
      "45\n",
      "False\n",
      "['▁K', 'w', 'y', 'j', 'i', 'b', 'o', \"'\", 's', '▁', 's', 'y', 'zy', 'g', 'y', '▁with', '▁Pr', 'zy', 'by', 's', 'ze', 'w', 'ski', \"'\", 's', '▁', 'psych', 'e', '▁caused', '▁', 'a', '▁hub', 'bu', 'b', '▁in', '▁Ou', 'aga', 'd', 'ou', 'go', 'u', '.']\n",
      "43\n",
      "['▁DHAH0', '▁B', 'U', 'H0', 'R', 'Z', 'H', 'W', 'AA1', '▁F', 'AO1', 'KS', '▁P', 'AA1', 'Z', '▁AE1T', '▁DHAH0', '▁R', 'AA1', 'ND', 'IH0', 'V', 'UW', '2', '▁K', 'AA1', 'Z', 'D', '▁K', 'W', 'AY1T', '▁AH0', '▁BR', 'UW1', 'HH', 'AA', '0', 'HH', 'AA', '0', '▁AH0', 'M', 'AH1', 'NG', 'ST', '▁DHAH0', '▁HH', 'OY1', '▁P', 'AA2', 'L', 'OY1', '.']\n",
      "53\n",
      "False\n",
      "['▁The', '▁', 'bourgeois', '▁faux', '▁pas', '▁at', '▁the', '▁rendez', 'vous', '▁caused', '▁quite', '▁', 'a', '▁bro', 'u', 'ha', 'ha', '▁among', 's', 't', '▁the', '▁ho', 'i', '▁poll', 'o', 'i', '.']\n",
      "28\n",
      "['▁N', 'IY1T', 'SH', 'IH0', 'Z', '▁AH0B', 'ER1', 'M', 'AH0N', 'SH', '▁TH', 'IH1', 'RIY0', '▁KL', 'AE1', 'SH', 'T', '▁WIH1DH', '▁SH', 'R', 'OW1', 'D', 'IH0NG', 'ER0Z', '▁K', 'W', 'AA1', 'NT', 'AH0M', '▁D', 'AA1P', 'IH0', 'L', 'G', 'AE', '0', 'ND', '▁HH', 'A', 'Y0', 'P', 'AA1T', 'H', 'AH0S', 'AH0S', '.']\n",
      "46\n",
      "False\n",
      "['▁Ni', 'etz', 'sche', \"'\", 's', '▁Über', 'men', 'sch', '▁theory', '▁clash', 'e', 'd', '▁with', '▁Sch', 'rö', 'd', 'inger', \"'\", 's', '▁quantum', '▁', 'd', 'oppel', 'gänge', 'r', '▁hypothesis', '.']\n",
      "28\n",
      "['▁DHAH0', '▁N', 'I', 'Y2', 'M', 'OW0', 'JH', 'AE', '2', 'N', 'K', 'OW0', 'S', 'EH2', 'R', 'AH0L', 'AH0N', 'EY1S', 'IY0', 'OW0', '▁D', 'AY2', 'AH0', 'G', 'N', 'OW1', 'S', 'AH0S', '▁FL', 'AH0M', 'AO1', 'K', 'ST', '▁DHAH0', '▁', 'OW0', 'T', 'OW2', 'RIY0', 'N', 'HH', 'AH0L', 'IH1', 'JH', 'AH0', 'SHAH0N', 'S', '.']\n",
      "48\n",
      "False\n",
      "['▁The', '▁pneu', 'mon', 'oul', 'tra', 'mic', 'r', 'oscopic', 'sili', 'co', 'vol', 'can', 'o', 'con', 'i', 'o', 's', 'is', '▁diagnosis', '▁flu', 'mm', 'o', 'x', 'e', 'd', '▁the', '▁', 'o', 't', 'or', 'hin', 'olar', 'y', 'ng', 'ologist', '.']\n",
      "37\n",
      "['▁N', 'AH0', 'K', 'YUW1', 'L', 'V', 'AE', '2', 'NG', 'K', '▁JH', 'AA1', 'K', 'OW0', 'M', 'OW2', 'Z', '▁P', 'IH0N', 'S', 'EH1N', 'EH0', 'Z', '▁G', 'L', 'IH1', 'NT', 'IH0', 'D', '▁AE1Z', '▁HHIY1', '▁P', 'ER0', 'UW1', 'Z', 'D', '▁GOW1', 'D', 'H', 'Z', '▁', 'UW1', 'V', 'R', 'AH0', '▁AA1N', '▁DHAH0', '▁CH', 'AE1', 'MP', 'S', 'AH0L', 'AH0S', 'AH0S', '.']\n",
      "55\n",
      "False\n",
      "['▁G', 'noc', 'chi', '-', 'lov', 'ing', '▁Gi', 'a', 'com', 'o', \"'\", 's', '▁pin', 'ce', '-', 'nez', '▁', 'gli', 'n', 'ted', '▁as', '▁', 'he', '▁per', 'used', '▁Go', 'e', 'the', \"'\", 's', '▁', 'oeuvre', '▁on', '▁the', '▁Champ', 's', '-', 'É', 'ly', 's', 'ées', '.']\n",
      "43\n",
      "['▁DHAH0', '▁T', 'S', 'K', 'AE1', 'NT', 'S', 'IY0', '▁AH1V', '▁DHAH0', '▁T', 'AA1R', 'M', 'IH0', 'G', 'AH0N', 'Z', '▁W', 'IH1NG', 'Z', '▁EH1', 'K', 'OW0', 'D', '▁TH', 'R', 'UW1', '▁DHAH0', '▁CH', 'ER1', 'CH', '▁AE1Z', '▁IH1T', '▁FL', 'UW1', '▁OW1', 'VER0', '▁L', 'AA1', 'K', '▁N', 'EH1S', '.']\n",
      "43\n",
      "False\n",
      "['▁The', '▁', 't', 's', 'k', 't', 's', 'k', '▁of', '▁the', '▁', 'p', 't', 'arm', 'igan', \"'\", 's', '▁wings', '▁', 'echoe', 'd', '▁through', '▁the', '▁', 'c', 'w', 't', 'ch', '▁as', '▁it', '▁fle', 'w', '▁over', '▁Loch', '▁Ne', 's', 's', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"The phthisic sphinx stealthily eschews the mnemonic pneumatic knickknacks.\",\n",
    "    \"Szczecin's squirrel chirped rhythmically while munching on zwitterionic quetzal quills.\",\n",
    "    \"Bjorn's fjord-dwelling lynx phthalate psyche was quite a conundrum.\",\n",
    "    \"The zeitgeist of Györgyike's csárdás xylophone quintet was truly unique.\",\n",
    "    \"Tsar Xylophone's czarina whispered žuželjka while eating sauerkraut in Århus.\",\n",
    "    \"Queuing for Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch's onomatopoeic eisteddfod.\",\n",
    "    \"The kleptomaniac's schwannoma diagnosis left the psychiatrist nonplussed.\",\n",
    "    \"Xerxes' xiphosura exhibited xenophobic behavior towards Zsa Zsa's Zhivago zhuzh.\",\n",
    "    \"The phlegmatic pharmacist's emphysema was exacerbated by Phthonus' chthonic chrysanthemums.\",\n",
    "    \"Kwyjibo's syzygy with Przybyszewski's psyche caused a hubbub in Ouagadougou.\",\n",
    "    \"The bourgeois faux pas at the rendezvous caused quite a brouhaha amongst the hoi polloi.\",\n",
    "    \"Nietzsche's Übermensch theory clashed with Schrödinger's quantum doppelgänger hypothesis.\",\n",
    "    \"The pneumonoultramicroscopicsilicovolcanoconiosis diagnosis flummoxed the otorhinolaryngologist.\",\n",
    "    \"Gnocchi-loving Giacomo's pince-nez glinted as he perused Goethe's oeuvre on the Champs-Élysées.\",\n",
    "    \"The tsktsk of the ptarmigan's wings echoed through the cwtch as it flew over Loch Ness.\"\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(encoding.tokens)\n",
    "    print(len(encoding.tokens))\n",
    "    print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    print(parler_tokenizer.tokenize(test_text))\n",
    "    print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer_g2pen.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_g2pen\\\\tokenizer_config.json',\n",
       " 'tokenizer_g2pen\\\\special_tokens_map.json',\n",
       " 'tokenizer_g2pen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_g2pen.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained('tokenizer_g2pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(wrapped_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
