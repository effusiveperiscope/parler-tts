{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ce2273cfe84981a0e684e3dfd95eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be55beca2cec45a988f8764f2d297816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add15ed695bf41e5b03cd6351367563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depends on g2p-en\n",
    "from datasets import load_dataset\n",
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "generics_kb = load_dataset(\n",
    "    'community-datasets/generics_kb', name='generics_kb_best', split='train')\n",
    "ponyspeech_dataset = load_dataset(\n",
    "    'synthbot/pony-speech', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAH1T WAH1N AA1N WAH1N, LEH1TS KLIY1N IH1T. SOW1 LEH1TS GEH1T DAW1N\n"
     ]
    }
   ],
   "source": [
    "def phonemize(text):\n",
    "    spl = text.split()\n",
    "    l = ''\n",
    "    for s in spl:\n",
    "        p = [arp for arp in g2p(s) if arp != ' ']\n",
    "        arpabet_string = ''.join(p)\n",
    "        l += arpabet_string + ' '\n",
    "    return l.strip()\n",
    "print(phonemize('But one on one, let\\'s clean it. So let\\'s get down'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generic_kb wiki:  30%|███       | 73164/240000 [04:39<10:36, 262.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\vul/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [70], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(corpus_output_txt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(generics_kb\u001b[38;5;241m.\u001b[39mshuffle()\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mrange\u001b[39m(n)\n\u001b[0;32m      8\u001b[0m     )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneric_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneric_kb wiki\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mphonemize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(ponyspeech_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mponyspeech\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(phonemize(text)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [69], line 11\u001b[0m, in \u001b[0;36mphonemize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spl:\n\u001b[1;32m---> 11\u001b[0m     p \u001b[38;5;241m=\u001b[39m [arp \u001b[38;5;28;01mfor\u001b[39;00m arp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mg2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m arp \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m     arpabet_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(p)\n\u001b[0;32m     13\u001b[0m     l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m arpabet_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\g2p_en\\g2p.py:162\u001b[0m, in \u001b[0;36mG2p.__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# tokenization\u001b[39;00m\n\u001b[0;32m    161\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m--> 162\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# tuples of (word, tag)\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# steps\u001b[39;00m\n\u001b[0;32m    165\u001b[0m prons \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_ \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_ \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "n = 240000\n",
    "corpus_output_txt = 'g2pen_corpus.txt'\n",
    "with open(corpus_output_txt, 'w', encoding='utf-8') as f:\n",
    "    for text in tqdm(generics_kb.shuffle().select(\n",
    "        range(n)\n",
    "    )['generic_sentence'], desc='generic_kb wiki'):\n",
    "        f.write(phonemize(text)+'\\n')\n",
    "    for text in tqdm(ponyspeech_dataset['transcription'], desc='ponyspeech'):\n",
    "        f.write(phonemize(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers import (decoders, models, normalizers,\n",
    "    pre_tokenizers, processors, trainers, Tokenizer,\n",
    "    )\n",
    "\n",
    "import re\n",
    "\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "    pre_tokenizers.ByteLevel(\n",
    "        add_prefix_space=True,\n",
    "        use_regex=False\n",
    "    ),\n",
    "    pre_tokenizers.Punctuation()\n",
    "    ])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.UnigramTrainer(vocab_size=1024,\n",
    "    special_tokens=special_tokens, unk_token='[UNK]')\n",
    "tokenizer.train(['g2pen_corpus_nosep.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ĠSĠOW0', (0, 5))]\n",
      "['Ġ', 'SĠ', 'OW0']\n",
      " S OW0\n"
     ]
    }
   ],
   "source": [
    "text = \"S OW0\"\n",
    "print(tokenizer.pre_tokenizer.pre_tokenize_str(text))\n",
    "print(tokenizer.encode(text).tokens)\n",
    "print(tokenizer.decode(tokenizer.encode(text).ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁Oh', (0, 2)), ('▁my', (2, 5)), ('▁god,', (5, 10)), ('▁he', (10, 13)), ('▁went', (13, 18)), ('▁to', (18, 21)), ('▁eat', (21, 25)), ('▁the', (25, 29)), ('▁local', (29, 35)), ('▁food', (35, 40)), ('▁and', (40, 44)), ('▁got', (44, 48)), ('▁really', (48, 55)), ('▁sick.', (55, 61))]\n",
      "['▁Oh', '▁my', '▁god', ',', '▁', 'he', '▁went', '▁to', '▁', 'eat', '▁the', '▁local', '▁food', '▁and', '▁got', '▁really', '▁sick', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "parler_tokenizer = Tokenizer.from_file('parler_tokenizer.json')\n",
    "print(parler_tokenizer.pre_tokenizer.pre_tokenize_str(\n",
    "    \"Oh my god, he went to eat the local food and got really sick.\"\n",
    "))\n",
    "print(parler_tokenizer.encode(\n",
    "    \"Oh my god, he went to eat the local food and got really sick.\"\n",
    ").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "parler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need some money, can I get some money? I need one million.\n",
      "['ĠAY1Ġ', 'NIY1DĠ', 'SAH1MĠ', 'M', 'AH1N', 'IY0', ',', 'Ġ', 'KAE1NĠ', 'AY1Ġ', 'GEH1TĠ', 'SAH1MĠ', 'M', 'AH1N', 'IY0', '?', 'ĠAY1Ġ', 'NIY1DĠ', 'WAH1NĠ', 'M', 'IH1L', 'Y', 'AH0N', '.']\n",
      "Raise up, get yourself together, and drive that funky soul.\n",
      "['Ġ', 'REY1', 'ZĠ', 'AH1', 'P', ',', 'Ġ', 'GEH1TĠ', 'YER0', 'SEH1LF', 'Ġ', 'TAH0', 'G', 'EH1', 'DH', 'ER0', ',', 'ĠAH0NDĠ', 'DR', 'AY1VĠ', 'DHAE1TĠ', 'F', 'AH1NGK', 'IY0Ġ', 'S', 'OW1L', '.']\n",
      "Our results demonstrate high fidelity speech generation and a diverse range of accents\n",
      "['Ġ', 'AW1ER0Ġ', 'RIH0ZAH1LTSĠ', 'D', 'EH1M', 'AH0N', 'STR', 'EY2TĠ', 'HHAY1', 'Ġ', 'F', 'AH0D', 'EH1L', 'AH0TIY0Ġ', 'SP', 'IY1CHĠ', 'JH', 'EH2N', 'ER0', 'EY1SHAH0NĠ', 'AH0NDĠ', 'AH0Ġ', 'D', 'AY0', 'VER1', 'SĠ', 'REY1N', 'JH', 'Ġ', 'AH1VĠ', 'AE1K', 'S', 'EH0N', 'T', 'S']\n",
      "Equestria the brave Equestrian.\n",
      "['Ġ', 'IH', '0KWEH1STRIY0AH0Ġ', 'DHAH0Ġ', 'B', 'REY1', 'V', 'Ġ', 'IH0K', 'W', 'EH1ST', 'RIY0', 'AH0N', '.']\n",
      "Dead nirik storage.\n",
      "['Ġ', 'D', 'EH1', 'DĠ', 'N', 'IH1R', 'IH0KĠ', 'ST', 'AO1R', 'AH0JH', '.']\n",
      "Well, one on one, let's clean it!\n",
      "['Ġ', 'WEH1L', ',', 'Ġ', 'WAH1NĠ', 'AA1NĠ', 'W', 'AH1N', ',', 'Ġ', 'LEH1', 'TSĠ', 'K', 'LIY1', 'NĠ', 'IH1T', '!']\n",
      "Amazon delivers packages quickly across the United States!\n",
      "['Ġ', 'AE1M', 'AH0Z', 'AA2N', 'Ġ', 'DIH0', 'LIH1', 'V', 'ER0ZĠ', 'P', 'AE1K', 'AH0JH', 'AH0ZĠ', 'KW', 'IH1K', 'LIY0Ġ', 'AH0', 'KR', 'AO1', 'SĠ', 'DHAH0ĠYUW0NAY1T', 'AH0DĠ', 'ST', 'EY1', 'T', 'S', '!']\n",
      "Bitcoin and Ethereum are popular cryptocurrencies.\n",
      "['Ġ', 'B', 'IH1T', 'KAH0N', 'ĠAH0NDĠ', 'EH1', 'TH', 'R', 'IY0AH0MĠ', 'AA1RĠ', 'PAA1PYAH0LER0Ġ', 'K', 'RIH1P', 'K', 'AH0', 'Y', 'ER2', 'AH0S', 'N', 'IY0', 'Z', '.']\n",
      "Text-to-speech models 1 $200\n",
      "['Ġ', 'T', 'EH1K', 'ST', 'SP', 'EH2K', 'Ġ', 'M', 'AA1', 'D', 'AH0LZĠ', 'WAH1NĠ', 'TUW1', 'HH', 'AH1N', 'DR', 'AH0D', 'D', 'AA1L', 'ER0', 'Z']\n",
      "Queue queuing\n",
      "['Ġ', 'K', 'YUW1Ġ', 'K', 'YUW1', 'IH0N', 'G']\n",
      "Twilight Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash\n",
      "['Ġ', 'TWAY1LAY2TĠ', 'SP', 'AA1R', 'K', 'AH0LĠ', 'P', 'IH1NGK', 'IY0Ġ', 'PAY1', 'Ġ', 'FLAH1TER0SHIY0Ġ', 'AE1PAH0LGAE0KĠ', 'R', 'EH1R', 'AH0TIY0Ġ', 'REY1NBOW2ĠDAE1SH']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing',\n",
    "    'Twilight Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash'\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(test_text)\n",
    "    print(encoding.tokens)\n",
    "    #print(len(encoding.tokens))\n",
    "    #print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    #print(parler_tokenizer.tokenize(test_text))\n",
    "    #print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠDHAH0Ġ', 'F', 'AH0', 'TH', 'IH1S', 'IH0KĠ', 'S', 'F', 'IH1NGK', 'SĠ', 'ST', 'EH1L', 'TH', 'IY0', 'LIY0Ġ', 'EH0', 'S', 'CH', 'UW1', 'ZĠ', 'DHAH0Ġ', 'N', 'IH0M', 'AA1N', 'IH0KĠ', 'N', 'UW0', 'MAE1', 'TIH0KĠ', 'N', 'IH1K', 'N', 'AE2K', 'S', '.']\n",
      "35\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'this', 'ic', '▁', 's', 'phin', 'x', '▁steal', 'thi', 'ly', '▁', 'e', 'sche', 'w', 's', '▁the', '▁', 'm', 'nem', 'onic', '▁pneu', 'matic', '▁', 'k', 'nick', 'k', 'n', 'ack', 's', '.']\n",
      "34\n",
      "['Ġ', 'SHIY1', 'S', 'IH0N', 'ZĠ', 'SK', 'WER1', 'AH0LĠ', 'CH', 'E', 'R', '1', 'P', 'TĠ', 'R', 'IH1', 'DH', 'M', 'IH0K', 'LIY0Ġ', 'WAY1', 'LĠ', 'M', 'AH1N', 'CH', 'IH0NGĠ', 'AA1NĠ', 'Z', 'W', 'IH2', 'TER0', 'SHIY1', 'N', 'IH0NGĠ', 'KW', 'EH1T', 'Z', 'AH0LĠ', 'KW', 'IH1L', 'Z', '.']\n",
      "42\n",
      "False\n",
      "['▁S', 'z', 'c', 'zeci', 'n', \"'\", 's', '▁squirrel', '▁', 'chir', 'ped', '▁rhythm', 'ically', '▁while', '▁mun', 'ching', '▁on', '▁', 'z', 'wit', 'terio', 'nic', '▁qu', 'etz', 'al', '▁qui', 'll', 's', '.']\n",
      "30\n",
      "['Ġ', 'B', 'AO1R', 'NZĠ', 'FER0', 'G', 'EH1', 'D', 'L', 'HH', 'IH0NĠ', 'L', 'IH1NGK', 'SĠ', 'F', 'EH1', 'TH', 'AH0L', 'EY2TĠ', 'SAY1', 'K', 'IY0Ġ', 'WAA1ZĠ', 'K', 'WAY1TĠ', 'AH0Ġ', 'KAH0N', 'AH1N', 'DR', 'AH0M', '.']\n",
      "31\n",
      "False\n",
      "['▁B', 'jor', 'n', \"'\", 's', '▁', 'f', 'jor', 'd', '-', 'd', 'well', 'ing', '▁', 'ly', 'n', 'x', '▁', 'phthal', 'ate', '▁', 'psych', 'e', '▁was', '▁quite', '▁', 'a', '▁con', 'und', 'rum', '.']\n",
      "32\n",
      "['ĠDHAH0Ġ', 'T', 'SAY1', 'T', 'G', 'AY2', 'ST', 'Ġ', 'AH1VĠ', 'JH', 'AO1R', 'G', 'IY0', 'IH0KS', 'IY0ZĠ', 'KAA1', 'S', 'R', 'AA0', 'DĠ', 'Z', 'AY1', 'L', 'AH0F', 'OW2N', 'Ġ', 'KW', 'IH0N', 'T', 'EH1T', 'Ġ', 'WAA1ZĠ', 'TR', 'UW1', 'LIY0Ġ', 'YUW0', 'N', 'IY1', 'K', '.']\n",
      "40\n",
      "False\n",
      "['▁The', '▁', 'zeit', 'ge', 'ist', '▁of', '▁Gy', 'ör', 'g', 'y', 'ike', \"'\", 's', '▁', 'c', 's', 'á', 'r', 'd', 'á', 's', '▁', 'x', 'y', 'l', 'o', 'phone', '▁qu', 'inte', 't', '▁was', '▁truly', '▁unique', '.']\n",
      "35\n",
      "['Ġ', 'Z', 'AA1RĠ', 'Z', 'AY1', 'L', 'AH0F', 'OW2N', 'ZĠ', 'CH', 'ER0', 'IY1', 'N', 'AH0Ġ', 'WIH1', 'SP', 'ER0DĠ', 'Z', 'UW2', 'Z', 'AH0G', 'LIY1', 'K', 'AH0Ġ', 'WAY1', 'LĠ', 'IY1', 'TIH0NGĠ', 'S', 'AW1', 'ER0', 'KR', 'AW2', 'TĠ', 'IH0NĠ', 'AA1R', 'HH', 'AH0S', '.']\n",
      "39\n",
      "False\n",
      "['▁T', 's', 'ar', '▁', 'X', 'y', 'l', 'o', 'phone', \"'\", 's', '▁', 'c', 'zar', 'in', 'a', '▁whisper', 'e', 'd', '▁', 'ž', 'u', 'ž', 'e', 'l', 'j', 'ka', '▁while', '▁eating', '▁sau', 'er', 'kraut', '▁in', '▁', 'Å', 'r', 'hus', '.']\n",
      "39\n",
      "['Ġ', 'K', 'YUW1', 'IH0NGĠ', 'FAO1RĠ', 'L', 'AE2N', 'F', 'OW0', 'DER0', 'K', 'OW0', 'L', 'Y', 'AE1', 'F', 'IH0KS', 'TĠ', 'AA2N', 'AH0M', 'OW0', 'TAH0', 'P', 'EY1', 'SH', 'IY0Ġ', 'AY1', 'ST', 'IH0', 'D', 'F', 'AO0', 'L', 'D', '.']\n",
      "35\n",
      "False\n",
      "['▁Que', 'u', 'ing', '▁for', '▁L', 'lan', 'fair', 'p', 'w', 'll', 'g', 'wyn', 'g', 'y', 'll', 'go', 'ger', 'ych', 'w', 'y', 'r', 'ndro', 'b', 'w', 'll', 'll', 'ant', 'y', 'sili', 'o', 'go', 'go', 'go', 'ch', \"'\", 's', '▁on', 'omato', 'p', 'o', 'e', 'ic', '▁', 'e', 'i', 'sted', 'd', 'f', 'o', 'd', '.']\n",
      "52\n",
      "['ĠDHAH0Ġ', 'KL', 'EH', '2', 'P', 'TAH0M', 'AA1N', 'AH0K', 'IY0ZĠ', 'SH', 'W', 'AH0N', 'AA1', 'M', 'AH0Ġ', 'D', 'AY2', 'AH0G', 'N', 'OW1', 'S', 'AH0SĠ', 'LEH1', 'F', 'TĠ', 'DHAH0Ġ', 'S', 'AH0K', 'AY1', 'AH0', 'TR', 'AH0STĠ', 'N', 'AA0', 'N', 'PL', 'AH1', 'ST', '.']\n",
      "39\n",
      "False\n",
      "['▁The', '▁', 'kle', 'p', 'to', 'mania', 'c', \"'\", 's', '▁', 'schw', 'an', 'nom', 'a', '▁diagnosis', '▁left', '▁the', '▁psychiatrist', '▁non', 'plus', 'sed', '.']\n",
      "23\n",
      "['Ġ', 'ZER1', 'K', 'S', 'IY0', 'Z', \"'\", 'Ġ', 'Z', 'IH0KS', 'W', 'AH0F', 'S', 'AO1R', 'AH0Ġ', 'IH0G', 'Z', 'IH1', 'B', 'AH0', 'TAH0DĠ', 'Z', 'EH2N', 'AH0F', 'AA1', 'B', 'IH0KĠ', 'BIH0HHEY1VYER0', 'Ġ', 'TAH0', 'WAO1R', 'DZĠ', 'ZH', 'AA1', 'Ġ', 'Z', 'AA1S', 'Ġ', 'ZH', 'IH0V', 'AA1', 'G', 'OW0', 'Ġ', 'Z', 'UW1', 'HH', 'IH0', '.']\n",
      "49\n",
      "False\n",
      "['▁', 'X', 'er', 'x', 'e', 's', \"'\", '▁', 'x', 'i', 'pho', 's', 'ura', '▁', 'exhibited', '▁', 'x', 'en', 'o', 'pho', 'bic', '▁behavior', '▁towards', '▁Z', 's', 'a', '▁Z', 's', 'a', \"'\", 's', '▁Z', 'h', 'iva', 'go', '▁', 'z', 'h', 'u', 'z', 'h', '.']\n",
      "43\n",
      "['ĠDHAH0Ġ', 'FL', 'EH0', 'G', 'MAE1', 'TIH0KĠ', 'F', 'AA1R', 'M', 'AH0S', 'IH0S', 'TSĠ', 'EH', '2', 'M', 'F', 'AH0Z', 'IY1', 'M', 'AH0Ġ', 'WAA1ZĠ', 'IH0G', 'Z', 'AE1S', 'ER0', 'B', 'EY2', 'TAH0DĠ', 'BAY1Ġ', 'F', 'EH1N', 'TH', 'AH0S', \"'\", 'Ġ', 'TH', 'AH0N', 'OW1', 'IH0KĠ', 'KR', 'IH0S', 'AE1N', 'TH', 'AH0M', 'AH0M', 'Z', '.']\n",
      "47\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'leg', 'matic', '▁pharmacist', \"'\", 's', '▁', 'e', 'm', 'phy', 's', 'e', 'm', 'a', '▁was', '▁', 'exacerbate', 'd', '▁by', '▁Ph', 'th', 'on', 'us', \"'\", '▁', 'ch', 'th', 'onic', '▁', 'ch', 'ry', 's', 'ant', 'hem', 'um', 's', '.']\n",
      "41\n",
      "['Ġ', 'KW', 'AH0JH', 'IY1', 'B', 'OW0', 'ZĠ', 'S', 'IH1Z', 'JH', 'IY0Ġ', 'WIH1DHĠ', 'PER0', 'Z', 'B', 'IH1G', 'Z', 'F', 'IY0ZĠ', 'SAY1', 'K', 'IY0Ġ', 'KAA1', 'Z', 'DĠ', 'AH0Ġ', 'HH', 'AH1', 'B', 'AH0B', 'Ġ', 'IH0NĠ', 'W', 'AA2', 'G', 'AH0D', 'UW1', 'G', 'UW0', '.']\n",
      "40\n",
      "False\n",
      "['▁K', 'w', 'y', 'j', 'i', 'b', 'o', \"'\", 's', '▁', 's', 'y', 'zy', 'g', 'y', '▁with', '▁Pr', 'zy', 'by', 's', 'ze', 'w', 'ski', \"'\", 's', '▁', 'psych', 'e', '▁caused', '▁', 'a', '▁hub', 'bu', 'b', '▁in', '▁Ou', 'aga', 'd', 'ou', 'go', 'u', '.']\n",
      "43\n",
      "['ĠDHAH0Ġ', 'B', 'U', 'H', '0', 'R', 'ZH', 'WAA1', 'Ġ', 'F', 'AO1', 'KSĠ', 'PAA1', 'ZĠ', 'AE1TĠ', 'DHAH0Ġ', 'RAA1', 'N', 'DIH0', 'V', 'UW2', 'Ġ', 'KAA1', 'Z', 'DĠ', 'K', 'WAY1TĠ', 'AH0Ġ', 'BR', 'UW1', 'HH', 'AA0', 'HH', 'AA0', 'Ġ', 'AH0M', 'AH1N', 'G', 'ST', 'ĠDHAH0Ġ', 'HH', 'OY1', 'Ġ', 'P', 'AA2', 'L', 'OY1', '.']\n",
      "48\n",
      "False\n",
      "['▁The', '▁', 'bourgeois', '▁faux', '▁pas', '▁at', '▁the', '▁rendez', 'vous', '▁caused', '▁quite', '▁', 'a', '▁bro', 'u', 'ha', 'ha', '▁among', 's', 't', '▁the', '▁ho', 'i', '▁poll', 'o', 'i', '.']\n",
      "28\n",
      "['Ġ', 'N', 'IY1', 'T', 'SH', 'IH0ZĠ', 'AH0', 'BER1', 'MAH0N', 'SH', 'Ġ', 'TH', 'IH1R', 'IY0Ġ', 'K', 'LAE1', 'SH', 'TĠ', 'WIH1DHĠ', 'SH', 'ROW1', 'D', 'IH0N', 'G', 'ER0ZĠ', 'KW', 'AA1N', 'TAH0M', 'Ġ', 'D', 'AA1', 'P', 'IH0L', 'G', 'AE0', 'N', 'DĠ', 'HH', 'AY0', 'PAA1', 'TH', 'AH0S', 'AH0S', '.']\n",
      "44\n",
      "False\n",
      "['▁Ni', 'etz', 'sche', \"'\", 's', '▁Über', 'men', 'sch', '▁theory', '▁clash', 'e', 'd', '▁with', '▁Sch', 'rö', 'd', 'inger', \"'\", 's', '▁quantum', '▁', 'd', 'oppel', 'gänge', 'r', '▁hypothesis', '.']\n",
      "28\n",
      "['ĠDHAH0Ġ', 'N', 'IY2', 'M', 'OW0', 'JH', 'AE2N', 'K', 'OW0', 'S', 'EH2R', 'AH0L', 'AH0N', 'EY1', 'S', 'IY0', 'OW0', 'Ġ', 'D', 'AY2', 'AH0G', 'N', 'OW1', 'S', 'AH0SĠ', 'FL', 'AH0M', 'AO1', 'K', 'ST', 'ĠDHAH0Ġ', 'OW0', 'T', 'OW2', 'RIY0', 'N', 'HH', 'AH0L', 'IH1', 'JH', 'AH0', 'SH', 'AH0N', 'S', '.']\n",
      "45\n",
      "False\n",
      "['▁The', '▁pneu', 'mon', 'oul', 'tra', 'mic', 'r', 'oscopic', 'sili', 'co', 'vol', 'can', 'o', 'con', 'i', 'o', 's', 'is', '▁diagnosis', '▁flu', 'mm', 'o', 'x', 'e', 'd', '▁the', '▁', 'o', 't', 'or', 'hin', 'olar', 'y', 'ng', 'ologist', '.']\n",
      "37\n",
      "['Ġ', 'N', 'AH0K', 'YUW1', 'L', 'V', 'AE2N', 'G', 'K', 'Ġ', 'JH', 'AA1K', 'OW0', 'M', 'OW2', 'ZĠ', 'P', 'IH0N', 'S', 'EH1N', 'EH0', 'ZĠ', 'G', 'L', 'IH1N', 'TIH0DĠ', 'AE1ZĠ', 'HHIY1', 'Ġ', 'PER0', 'UW1', 'Z', 'DĠ', 'G', 'OW1', 'DH', 'ZĠ', 'UW1', 'V', 'RAH0', 'Ġ', 'AA1NĠDHAH0Ġ', 'CH', 'AE1M', 'P', 'SAH0L', 'AH0S', 'AH0S', '.']\n",
      "49\n",
      "False\n",
      "['▁G', 'noc', 'chi', '-', 'lov', 'ing', '▁Gi', 'a', 'com', 'o', \"'\", 's', '▁pin', 'ce', '-', 'nez', '▁', 'gli', 'n', 'ted', '▁as', '▁', 'he', '▁per', 'used', '▁Go', 'e', 'the', \"'\", 's', '▁', 'oeuvre', '▁on', '▁the', '▁Champ', 's', '-', 'É', 'ly', 's', 'ées', '.']\n",
      "43\n",
      "['ĠDHAH0Ġ', 'T', 'SK', 'AE1N', 'T', 'S', 'IY0Ġ', 'AH1VĠDHAH0Ġ', 'T', 'AA1R', 'M', 'IH0G', 'AH0NZĠ', 'WIH1N', 'G', 'ZĠ', 'EH1K', 'OW0', 'DĠ', 'THRUW1Ġ', 'DHAH0Ġ', 'CH', 'E', 'R', '1', 'CH', 'Ġ', 'AE1ZĠ', 'IH1TĠ', 'FL', 'UW1', 'Ġ', 'OW1VER0Ġ', 'L', 'AA1K', 'Ġ', 'N', 'EH1S', '.']\n",
      "39\n",
      "False\n",
      "['▁The', '▁', 't', 's', 'k', 't', 's', 'k', '▁of', '▁the', '▁', 'p', 't', 'arm', 'igan', \"'\", 's', '▁wings', '▁', 'echoe', 'd', '▁through', '▁the', '▁', 'c', 'w', 't', 'ch', '▁as', '▁it', '▁fle', 'w', '▁over', '▁Loch', '▁Ne', 's', 's', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"The phthisic sphinx stealthily eschews the mnemonic pneumatic knickknacks.\",\n",
    "    \"Szczecin's squirrel chirped rhythmically while munching on zwitterionic quetzal quills.\",\n",
    "    \"Bjorn's fjord-dwelling lynx phthalate psyche was quite a conundrum.\",\n",
    "    \"The zeitgeist of Györgyike's csárdás xylophone quintet was truly unique.\",\n",
    "    \"Tsar Xylophone's czarina whispered žuželjka while eating sauerkraut in Århus.\",\n",
    "    \"Queuing for Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch's onomatopoeic eisteddfod.\",\n",
    "    \"The kleptomaniac's schwannoma diagnosis left the psychiatrist nonplussed.\",\n",
    "    \"Xerxes' xiphosura exhibited xenophobic behavior towards Zsa Zsa's Zhivago zhuzh.\",\n",
    "    \"The phlegmatic pharmacist's emphysema was exacerbated by Phthonus' chthonic chrysanthemums.\",\n",
    "    \"Kwyjibo's syzygy with Przybyszewski's psyche caused a hubbub in Ouagadougou.\",\n",
    "    \"The bourgeois faux pas at the rendezvous caused quite a brouhaha amongst the hoi polloi.\",\n",
    "    \"Nietzsche's Übermensch theory clashed with Schrödinger's quantum doppelgänger hypothesis.\",\n",
    "    \"The pneumonoultramicroscopicsilicovolcanoconiosis diagnosis flummoxed the otorhinolaryngologist.\",\n",
    "    \"Gnocchi-loving Giacomo's pince-nez glinted as he perused Goethe's oeuvre on the Champs-Élysées.\",\n",
    "    \"The tsktsk of the ptarmigan's wings echoed through the cwtch as it flew over Loch Ness.\"\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(encoding.tokens)\n",
    "    print(len(encoding.tokens))\n",
    "    print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    print(parler_tokenizer.tokenize(test_text))\n",
    "    print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer_g2p_v3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_g2p_v3\\\\tokenizer_config.json',\n",
       " 'tokenizer_g2p_v3\\\\special_tokens_map.json',\n",
       " 'tokenizer_g2p_v3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_g2p_v3.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained('tokenizer_g2p_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(wrapped_tokenizer.vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
