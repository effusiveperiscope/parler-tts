{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e3a05c025c40ef8b1e34680d0e400c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a396218059ef49a38e7bb84919250840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20096a59c18d447d8af765b3692b6bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depends on g2p-en\n",
    "from datasets import load_dataset\n",
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "generics_kb = load_dataset(\n",
    "    'community-datasets/generics_kb', name='generics_kb_best', split='train')\n",
    "ponyspeech_dataset = load_dataset(\n",
    "    'synthbot/pony-speech', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAH1T WAH1N AA1N WAH1N , LEH1TS KLIY1N IH1T .\n"
     ]
    }
   ],
   "source": [
    "horsewords = {}\n",
    "with open('horsewords.clean') as f:\n",
    "    while line := f.readline():\n",
    "        spl = line.split('  ')\n",
    "        horsewords[spl[0]] = spl[1]\n",
    "\n",
    "def phonemize(text):\n",
    "    spl = text.split()\n",
    "    l = []\n",
    "    for s in spl:\n",
    "        arpabet_string = ''.join(g2p(s))\n",
    "        l.append(arpabet_string)\n",
    "    return ' '.join(l)\n",
    "print(phonemize('But one on one, let\\'s clean it.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generic_kb wiki: 100%|██████████| 240000/240000 [09:42<00:00, 412.31it/s]\n",
      "ponyspeech: 100%|██████████| 64783/64783 [02:06<00:00, 511.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "n = 240000\n",
    "corpus_output_txt = 'g2pen_corpus_nosep.txt'\n",
    "with open(corpus_output_txt, 'w', encoding='utf-8') as f:\n",
    "    for text in tqdm(generics_kb.shuffle().select(\n",
    "        range(n)\n",
    "    )['generic_sentence'], desc='generic_kb wiki'):\n",
    "        f.write(phonemize(text)+'\\n')\n",
    "    for text in tqdm(ponyspeech_dataset['transcription'], desc='ponyspeech'):\n",
    "        f.write(phonemize(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers import (decoders, models, normalizers,\n",
    "    pre_tokenizers, processors, trainers, Tokenizer)\n",
    "\n",
    "import re\n",
    "\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "    pre_tokenizers.Whitespace(),\n",
    "    #pre_tokenizers.Split(r'\\s+', behavior='removed'), \n",
    "    pre_tokenizers.Punctuation(),\n",
    "    ])\n",
    "trainer = trainers.BpeTrainer(vocab_size=384,\n",
    "    special_tokens=special_tokens, unk_token='[UNK]')\n",
    "tokenizer.train(['g2pen_corpus_nosep.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "parler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AY1', 'N', 'IY1', 'D', 'SAH1M', 'M', 'AH1N', 'IY0', ',', 'KAE1N', 'AY1', 'G', 'EH1T', 'SAH1M', 'M', 'AH1N', 'IY0', '?', 'AY1', 'N', 'IY1', 'D', 'WAH1N', 'M', 'IH1L', 'Y', 'AH0N', '.']\n",
      "28\n",
      "False\n",
      "['▁I', '▁need', '▁some', '▁money', ',', '▁can', '▁I', '▁get', '▁some', '▁money', '?', '▁I', '▁need', '▁one', '▁million', '.']\n",
      "17\n",
      "['R', 'EY1Z', 'AH1P', ',', 'G', 'EH1T', 'Y', 'ER0', 'SEH1L', 'F', 'T', 'AH0G', 'EH1', 'DHER0', ',', 'AH0ND', 'DR', 'AY1', 'V', 'DHAE1T', 'F', 'AH1NG', 'K', 'IY0', 'S', 'OW1L', '.']\n",
      "27\n",
      "False\n",
      "['▁R', 'aise', '▁up', ',', '▁get', '▁yourself', '▁together', ',', '▁and', '▁drive', '▁that', '▁fun', 'ky', '▁soul', '.']\n",
      "16\n",
      "['AW1ER0', 'R', 'IH0Z', 'AH1L', 'TS', 'D', 'EH1', 'MAH0N', 'STR', 'EY2T', 'HHAY1', 'F', 'AH0D', 'EH1L', 'AH0TIY0', 'SP', 'IY1', 'CH', 'JH', 'EH2', 'N', 'ER0', 'EY1SHAH0N', 'AH0ND', 'AH0', 'D', 'AY0', 'V', 'ER1', 'S', 'R', 'EY1NJH', 'AH1V', 'AE1', 'KS', 'EH0', 'N', 'TS']\n",
      "38\n",
      "False\n",
      "['▁Our', '▁results', '▁demonstrate', '▁high', '▁', 'fidelity', '▁speech', '▁generation', '▁and', '▁', 'a', '▁diverse', '▁range', '▁of', '▁accent', 's']\n",
      "17\n",
      "['IH0K', 'W', 'EH1ST', 'RIY0', 'AH0', 'DHAH0', 'BR', 'EY1', 'V', 'IH0K', 'W', 'EH1ST', 'RIY0', 'AH0N', '.']\n",
      "15\n",
      "False\n",
      "['▁E', 'que', 'stria', '▁the', '▁brave', '▁E', 'quest', 'rian', '.']\n",
      "10\n",
      "['D', 'EH1D', 'N', 'IH1R', 'IH0K', 'ST', 'AO1R', 'AH0JH', '.']\n",
      "9\n",
      "False\n",
      "['▁Dead', '▁', 'n', 'i', 'rik', '▁storage', '.']\n",
      "8\n",
      "['WEH1L', ',', 'WAH1N', 'AA1N', 'WAH1N', ',', 'L', 'EH1', 'TS', 'KL', 'IY1N', 'IH1T', '!']\n",
      "13\n",
      "False\n",
      "['▁Well', ',', '▁one', '▁on', '▁one', ',', '▁let', \"'\", 's', '▁clean', '▁it', '!']\n",
      "13\n",
      "['AE1M', 'AH0Z', 'AA2', 'N', 'DIH0', 'LIH1', 'V', 'ER0Z', 'P', 'AE1K', 'AH0JH', 'AH0Z', 'K', 'WIH1', 'K', 'LIY0', 'AH0K', 'R', 'AO1', 'S', 'DHAH0', 'YUW0', 'N', 'AY1T', 'AH0D', 'ST', 'EY1', 'TS', '!']\n",
      "29\n",
      "False\n",
      "['▁Amazon', '▁delivers', '▁packages', '▁quickly', '▁across', '▁the', '▁United', '▁States', '!']\n",
      "10\n",
      "['B', 'IH1T', 'KAH0N', 'AH0ND', 'EH1', 'TH', 'RIY0', 'AH0M', 'AA1R', 'P', 'AA1', 'P', 'YAH0L', 'ER0', 'KR', 'IH1', 'P', 'K', 'AH0', 'Y', 'ER', '2', 'AH0S', 'N', 'IY0Z', '.']\n",
      "26\n",
      "False\n",
      "['▁Bitcoin', '▁and', '▁Ethereum', '▁are', '▁popular', '▁', 'cryptocurrencies', '.']\n",
      "9\n",
      "['T', 'EH1K', 'ST', 'SP', 'EH2', 'K', 'M', 'AA1', 'D', 'AH0LZ', 'WAH1N', 'TUW1', 'HH', 'AH1N', 'DR', 'AH0D', 'D', 'AA1L', 'ER0Z']\n",
      "19\n",
      "False\n",
      "['▁Text', '-', 'to', '-', 's', 'pe', 'ech', '▁models', '▁1', '▁$200']\n",
      "11\n",
      "['K', 'YUW1', 'K', 'YUW1', 'IH0NG']\n",
      "5\n",
      "False\n",
      "['▁Que', 'u', 'e', '▁que', 'u', 'ing']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing'\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(encoding.tokens)\n",
    "    print(len(encoding.tokens))\n",
    "    print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    print(parler_tokenizer.tokenize(test_text))\n",
    "    print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DHAH0', 'F', 'AH0T', 'H', 'IH1S', 'IH0K', 'S', 'F', 'IH1NG', 'KS', 'ST', 'EH1L', 'TH', 'IY0', 'LIY0', 'EH0', 'S', 'CH', 'UW1', 'Z', 'DHAH0', 'N', 'IH0', 'M', 'AA1N', 'IH0K', 'N', 'UW0', 'M', 'AE1T', 'IH0K', 'N', 'IH1K', 'N', 'AE2', 'KS', '.']\n",
      "37\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'this', 'ic', '▁', 's', 'phin', 'x', '▁steal', 'thi', 'ly', '▁', 'e', 'sche', 'w', 's', '▁the', '▁', 'm', 'nem', 'onic', '▁pneu', 'matic', '▁', 'k', 'nick', 'k', 'n', 'ack', 's', '.']\n",
      "34\n",
      "['SH', 'IY1', 'S', 'IH0N', 'Z', 'SK', 'WER1', 'AH0L', 'CH', 'ER1', 'PT', 'RIH1', 'DH', 'M', 'IH0K', 'LIY0', 'WAY1', 'L', 'M', 'AH1N', 'CH', 'IH0NG', 'AA1N', 'Z', 'W', 'IH2', 'TER0', 'SH', 'IY1N', 'IH0NG', 'KW', 'EH1T', 'Z', 'AH0L', 'K', 'WIH1', 'LZ', '.']\n",
      "38\n",
      "False\n",
      "['▁S', 'z', 'c', 'zeci', 'n', \"'\", 's', '▁squirrel', '▁', 'chir', 'ped', '▁rhythm', 'ically', '▁while', '▁mun', 'ching', '▁on', '▁', 'z', 'wit', 'terio', 'nic', '▁qu', 'etz', 'al', '▁qui', 'll', 's', '.']\n",
      "30\n",
      "['B', 'AO1R', 'NZ', 'FER0', 'G', 'EH1D', 'L', 'HH', 'IH0N', 'L', 'IH1NG', 'KS', 'F', 'EH1', 'TH', 'AH0L', 'EY2T', 'SAY1', 'K', 'IY0', 'WAA1', 'Z', 'K', 'WAY1', 'T', 'AH0', 'KAH0N', 'AH1N', 'DR', 'AH0M', '.']\n",
      "31\n",
      "False\n",
      "['▁B', 'jor', 'n', \"'\", 's', '▁', 'f', 'jor', 'd', '-', 'd', 'well', 'ing', '▁', 'ly', 'n', 'x', '▁', 'phthal', 'ate', '▁', 'psych', 'e', '▁was', '▁quite', '▁', 'a', '▁con', 'und', 'rum', '.']\n",
      "32\n",
      "['DHAH0', 'TS', 'AY1T', 'G', 'AY2', 'ST', 'AH1V', 'JH', 'AO1R', 'G', 'IY0', 'IH0KS', 'IY0Z', 'KAA1', 'S', 'R', 'AA', '0', 'D', 'Z', 'AY1', 'L', 'AH0F', 'OW2', 'N', 'KW', 'IH0N', 'T', 'EH1T', 'WAA1', 'Z', 'TR', 'UW1', 'LIY0', 'YUW0', 'N', 'IY1', 'K', '.']\n",
      "39\n",
      "False\n",
      "['▁The', '▁', 'zeit', 'ge', 'ist', '▁of', '▁Gy', 'ör', 'g', 'y', 'ike', \"'\", 's', '▁', 'c', 's', 'á', 'r', 'd', 'á', 's', '▁', 'x', 'y', 'l', 'o', 'phone', '▁qu', 'inte', 't', '▁was', '▁truly', '▁unique', '.']\n",
      "35\n",
      "['Z', 'AA1R', 'Z', 'AY1', 'L', 'AH0F', 'OW2', 'NZ', 'CHER0', 'IY1N', 'AH0', 'WIH1', 'SP', 'ER0', 'D', 'Z', 'UW2', 'Z', 'AH0G', 'LIY1', 'K', 'AH0', 'WAY1', 'L', 'IY1T', 'IH0NG', 'S', 'AW1ER0', 'KR', 'A', 'W2', 'T', 'IH0N', 'AA1R', 'HH', 'AH0S', '.']\n",
      "37\n",
      "False\n",
      "['▁T', 's', 'ar', '▁', 'X', 'y', 'l', 'o', 'phone', \"'\", 's', '▁', 'c', 'zar', 'in', 'a', '▁whisper', 'e', 'd', '▁', 'ž', 'u', 'ž', 'e', 'l', 'j', 'ka', '▁while', '▁eating', '▁sau', 'er', 'kraut', '▁in', '▁', 'Å', 'r', 'hus', '.']\n",
      "39\n",
      "['K', 'YUW1', 'IH0NG', 'FAO1R', 'L', 'AE2', 'N', 'F', 'OW0', 'DER0', 'K', 'OW0', 'L', 'Y', 'AE1', 'F', 'IH0K', 'ST', 'AA2', 'N', 'AH0M', 'OW0', 'T', 'AH0P', 'EY1', 'SH', 'IY0', 'AY1', 'ST', 'IH0D', 'F', 'AO', '0', 'LD', '.']\n",
      "35\n",
      "False\n",
      "['▁Que', 'u', 'ing', '▁for', '▁L', 'lan', 'fair', 'p', 'w', 'll', 'g', 'wyn', 'g', 'y', 'll', 'go', 'ger', 'ych', 'w', 'y', 'r', 'ndro', 'b', 'w', 'll', 'll', 'ant', 'y', 'sili', 'o', 'go', 'go', 'go', 'ch', \"'\", 's', '▁on', 'omato', 'p', 'o', 'e', 'ic', '▁', 'e', 'i', 'sted', 'd', 'f', 'o', 'd', '.']\n",
      "52\n",
      "['DHAH0', 'KL', 'EH2', 'PT', 'AH0M', 'AA1N', 'AH0K', 'IY0Z', 'SH', 'W', 'AH0N', 'AA1', 'M', 'AH0', 'D', 'AY2', 'AH0G', 'NOW1', 'S', 'AH0S', 'L', 'EH1', 'F', 'T', 'DHAH0', 'S', 'AH0K', 'AY1', 'AH0T', 'R', 'AH0ST', 'N', 'AA', '0', 'N', 'PL', 'AH1ST', '.']\n",
      "38\n",
      "False\n",
      "['▁The', '▁', 'kle', 'p', 'to', 'mania', 'c', \"'\", 's', '▁', 'schw', 'an', 'nom', 'a', '▁diagnosis', '▁left', '▁the', '▁psychiatrist', '▁non', 'plus', 'sed', '.']\n",
      "23\n",
      "['Z', 'ER1', 'KS', 'IY0Z', \"'\", 'Z', 'IH0KS', 'W', 'AH0F', 'S', 'AO1R', 'AH0', 'IH0', 'GZ', 'IH1', 'B', 'AH0T', 'AH0D', 'Z', 'EH2', 'N', 'AH0F', 'AA1', 'BIH0K', 'BIH0', 'HH', 'EY1', 'V', 'Y', 'ER0', 'T', 'AH0W', 'AO1R', 'DZ', 'Z', 'H', 'AA1', 'Z', 'AA1', 'S', 'Z', 'H', 'IH0V', 'AA1', 'G', 'OW0', 'Z', 'UW1', 'HH', 'IH0', '.']\n",
      "51\n",
      "False\n",
      "['▁', 'X', 'er', 'x', 'e', 's', \"'\", '▁', 'x', 'i', 'pho', 's', 'ura', '▁', 'exhibited', '▁', 'x', 'en', 'o', 'pho', 'bic', '▁behavior', '▁towards', '▁Z', 's', 'a', '▁Z', 's', 'a', \"'\", 's', '▁Z', 'h', 'iva', 'go', '▁', 'z', 'h', 'u', 'z', 'h', '.']\n",
      "43\n",
      "['DHAH0', 'FL', 'EH0', 'G', 'M', 'AE1T', 'IH0K', 'F', 'AA1R', 'M', 'AH0S', 'IH0', 'ST', 'S', 'EH2', 'M', 'F', 'AH0Z', 'IY1', 'M', 'AH0', 'WAA1', 'Z', 'IH0', 'GZ', 'AE1S', 'ER0', 'B', 'EY2TAH0D', 'BAY1', 'F', 'EH1N', 'TH', 'AH0S', \"'\", 'TH', 'AH0N', 'OW1', 'IH0K', 'KR', 'IH0', 'S', 'AE1N', 'TH', 'AH0M', 'AH0MZ', '.']\n",
      "47\n",
      "False\n",
      "['▁The', '▁', 'p', 'h', 'leg', 'matic', '▁pharmacist', \"'\", 's', '▁', 'e', 'm', 'phy', 's', 'e', 'm', 'a', '▁was', '▁', 'exacerbate', 'd', '▁by', '▁Ph', 'th', 'on', 'us', \"'\", '▁', 'ch', 'th', 'onic', '▁', 'ch', 'ry', 's', 'ant', 'hem', 'um', 's', '.']\n",
      "41\n",
      "['KW', 'AH0JH', 'IY1', 'B', 'OW0', 'Z', 'S', 'IH1Z', 'JH', 'IY0', 'WIH1DH', 'P', 'ER0Z', 'B', 'IH1', 'GZ', 'F', 'IY0Z', 'SAY1', 'K', 'IY0', 'KAA1Z', 'D', 'AH0', 'HH', 'AH1', 'B', 'AH0B', 'IH0N', 'W', 'AA2', 'G', 'AH0D', 'UW1', 'G', 'UW0', '.']\n",
      "37\n",
      "False\n",
      "['▁K', 'w', 'y', 'j', 'i', 'b', 'o', \"'\", 's', '▁', 's', 'y', 'zy', 'g', 'y', '▁with', '▁Pr', 'zy', 'by', 's', 'ze', 'w', 'ski', \"'\", 's', '▁', 'psych', 'e', '▁caused', '▁', 'a', '▁hub', 'bu', 'b', '▁in', '▁Ou', 'aga', 'd', 'ou', 'go', 'u', '.']\n",
      "43\n",
      "['DHAH0', 'B', 'U', 'H0', 'R', 'Z', 'H', 'WAA1', 'F', 'AO1', 'KS', 'P', 'AA1', 'Z', 'AE1T', 'DHAH0', 'R', 'AA1N', 'DIH0', 'V', 'UW2', 'KAA1Z', 'D', 'K', 'WAY1', 'T', 'AH0', 'BR', 'UW1', 'HH', 'AA', '0', 'HH', 'AA', '0', 'AH0M', 'AH1NG', 'ST', 'DHAH0', 'HH', 'OY1', 'P', 'AA2', 'L', 'OY1', '.']\n",
      "46\n",
      "False\n",
      "['▁The', '▁', 'bourgeois', '▁faux', '▁pas', '▁at', '▁the', '▁rendez', 'vous', '▁caused', '▁quite', '▁', 'a', '▁bro', 'u', 'ha', 'ha', '▁among', 's', 't', '▁the', '▁ho', 'i', '▁poll', 'o', 'i', '.']\n",
      "28\n",
      "['N', 'IY1T', 'SH', 'IH0Z', 'AH0B', 'ER1', 'MAH0N', 'SH', 'TH', 'IH1RIY0', 'KL', 'AE1', 'SH', 'T', 'WIH1DH', 'SH', 'R', 'OW1', 'D', 'IH0NG', 'ER0Z', 'KW', 'AA1N', 'T', 'AH0M', 'D', 'AA1', 'P', 'IH0L', 'G', 'AE0', 'ND', 'HH', 'AY0', 'P', 'AA1', 'TH', 'AH0S', 'AH0S', '.']\n",
      "40\n",
      "False\n",
      "['▁Ni', 'etz', 'sche', \"'\", 's', '▁Über', 'men', 'sch', '▁theory', '▁clash', 'e', 'd', '▁with', '▁Sch', 'rö', 'd', 'inger', \"'\", 's', '▁quantum', '▁', 'd', 'oppel', 'gänge', 'r', '▁hypothesis', '.']\n",
      "28\n",
      "['DHAH0', 'N', 'IY2', 'M', 'OW0', 'JH', 'AE2', 'N', 'K', 'OW0', 'SEH2', 'R', 'AH0L', 'AH0N', 'EY1S', 'IY0', 'OW0', 'D', 'AY2', 'AH0G', 'NOW1', 'S', 'AH0S', 'FL', 'AH0M', 'AO1', 'K', 'ST', 'DHAH0', 'OW0', 'T', 'OW2', 'RIY0', 'N', 'HH', 'AH0L', 'IH1', 'JH', 'AH0', 'SHAH0N', 'S', '.']\n",
      "42\n",
      "False\n",
      "['▁The', '▁pneu', 'mon', 'oul', 'tra', 'mic', 'r', 'oscopic', 'sili', 'co', 'vol', 'can', 'o', 'con', 'i', 'o', 's', 'is', '▁diagnosis', '▁flu', 'mm', 'o', 'x', 'e', 'd', '▁the', '▁', 'o', 't', 'or', 'hin', 'olar', 'y', 'ng', 'ologist', '.']\n",
      "37\n",
      "['N', 'AH0K', 'YUW1', 'L', 'V', 'AE2', 'NG', 'K', 'JH', 'AA1', 'K', 'OW0', 'M', 'OW2', 'Z', 'P', 'IH0N', 'S', 'EH1N', 'EH0', 'Z', 'G', 'L', 'IH1N', 'T', 'IH0D', 'AE1Z', 'HHIY1', 'PER0', 'UW1', 'Z', 'D', 'G', 'OW1', 'DH', 'Z', 'UW1', 'V', 'R', 'AH0', 'AA1N', 'DHAH0', 'CH', 'AE1', 'MP', 'S', 'AH0L', 'AH0S', 'AH0S', '.']\n",
      "50\n",
      "False\n",
      "['▁G', 'noc', 'chi', '-', 'lov', 'ing', '▁Gi', 'a', 'com', 'o', \"'\", 's', '▁pin', 'ce', '-', 'nez', '▁', 'gli', 'n', 'ted', '▁as', '▁', 'he', '▁per', 'used', '▁Go', 'e', 'the', \"'\", 's', '▁', 'oeuvre', '▁on', '▁the', '▁Champ', 's', '-', 'É', 'ly', 's', 'ées', '.']\n",
      "43\n",
      "['DHAH0', 'TS', 'KAE1N', 'TS', 'IY0', 'AH1V', 'DHAH0', 'T', 'AA1R', 'M', 'IH0', 'G', 'AH0NZ', 'W', 'IH1NG', 'Z', 'EH1K', 'OW0', 'D', 'THR', 'UW1', 'DHAH0', 'CH', 'ER1', 'CH', 'AE1Z', 'IH1T', 'FL', 'UW1', 'OW1', 'VER0', 'L', 'AA1', 'K', 'N', 'EH1S', '.']\n",
      "37\n",
      "False\n",
      "['▁The', '▁', 't', 's', 'k', 't', 's', 'k', '▁of', '▁the', '▁', 'p', 't', 'arm', 'igan', \"'\", 's', '▁wings', '▁', 'echoe', 'd', '▁through', '▁the', '▁', 'c', 'w', 't', 'ch', '▁as', '▁it', '▁fle', 'w', '▁over', '▁Loch', '▁Ne', 's', 's', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"The phthisic sphinx stealthily eschews the mnemonic pneumatic knickknacks.\",\n",
    "    \"Szczecin's squirrel chirped rhythmically while munching on zwitterionic quetzal quills.\",\n",
    "    \"Bjorn's fjord-dwelling lynx phthalate psyche was quite a conundrum.\",\n",
    "    \"The zeitgeist of Györgyike's csárdás xylophone quintet was truly unique.\",\n",
    "    \"Tsar Xylophone's czarina whispered žuželjka while eating sauerkraut in Århus.\",\n",
    "    \"Queuing for Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch's onomatopoeic eisteddfod.\",\n",
    "    \"The kleptomaniac's schwannoma diagnosis left the psychiatrist nonplussed.\",\n",
    "    \"Xerxes' xiphosura exhibited xenophobic behavior towards Zsa Zsa's Zhivago zhuzh.\",\n",
    "    \"The phlegmatic pharmacist's emphysema was exacerbated by Phthonus' chthonic chrysanthemums.\",\n",
    "    \"Kwyjibo's syzygy with Przybyszewski's psyche caused a hubbub in Ouagadougou.\",\n",
    "    \"The bourgeois faux pas at the rendezvous caused quite a brouhaha amongst the hoi polloi.\",\n",
    "    \"Nietzsche's Übermensch theory clashed with Schrödinger's quantum doppelgänger hypothesis.\",\n",
    "    \"The pneumonoultramicroscopicsilicovolcanoconiosis diagnosis flummoxed the otorhinolaryngologist.\",\n",
    "    \"Gnocchi-loving Giacomo's pince-nez glinted as he perused Goethe's oeuvre on the Champs-Élysées.\",\n",
    "    \"The tsktsk of the ptarmigan's wings echoed through the cwtch as it flew over Loch Ness.\"\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    #print(tokenizer.pre_tokenizer.pre_tokenize_str(phonemize(test_text)))\n",
    "    encoding = tokenizer.encode(phonemize(test_text))\n",
    "    print(encoding.tokens)\n",
    "    print(len(encoding.tokens))\n",
    "    print(any([tok == '[UNK]' for tok in encoding.tokens]))\n",
    "\n",
    "    print(parler_tokenizer.tokenize(test_text))\n",
    "    print(len(parler_tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer_g2pen.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_g2pen\\\\tokenizer_config.json',\n",
       " 'tokenizer_g2pen\\\\special_tokens_map.json',\n",
       " 'tokenizer_g2pen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_g2pen.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained('tokenizer_g2pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(wrapped_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
