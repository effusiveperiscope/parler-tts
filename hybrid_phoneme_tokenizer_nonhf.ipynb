{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Credit to Synthbot for pruned tokenizers\n",
    "class HybridPhonemeTokenizer:\n",
    "    def __init__(self,\n",
    "        tokenizer_eng = 'synthbot/parlertts_tokenizer_clean',\n",
    "        tokenizer_g2p = 'synthbot/vul_g2pen_tokenizer_clean',\n",
    "        eng_special = {\n",
    "            'pad_token': \"<pad>\",\n",
    "            'sep_token': \"</s>\",\n",
    "            'unk_token': \"<unk>\",\n",
    "        },\n",
    "        g2p_special = {\n",
    "            'unk_token': \"[UNK]\",\n",
    "            'pad_token': \"[PAD]\",\n",
    "            'cls_token': \"[CLS]\",\n",
    "            'sep_token': \"[SEP]\",\n",
    "            'mask_token':\"[MASk]\",\n",
    "        },\n",
    "         **kwargs):\n",
    "        self.name_or_path = 'hybrid_phoneme_tokenizer'\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_eng, **eng_special)\n",
    "        self.tokenizer_g2p = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_g2p, **g2p_special)\n",
    "\n",
    "        # Not sure if this is actually necessary - ByteLevel pretokenizer\n",
    "        # removes possibility of <unk> tokens\n",
    "        self.special_tokens = {\n",
    "            self.tokenizer_g2p.pad_token_id: self.tokenizer_eng.pad_token_id,\n",
    "            self.tokenizer_g2p.bos_token_id: self.tokenizer_eng.bos_token_id,\n",
    "            self.tokenizer_g2p.cls_token_id: self.tokenizer_eng.cls_token_id,\n",
    "            self.tokenizer_g2p.eos_token_id: self.tokenizer_eng.eos_token_id,\n",
    "            self.tokenizer_g2p.unk_token_id: self.tokenizer_eng.unk_token_id,\n",
    "            self.tokenizer_g2p.mask_token_id: self.tokenizer_eng.mask_token_id\n",
    "        }\n",
    "\n",
    "        g2p_offset = len(self.tokenizer_eng.get_vocab())\n",
    "        self.g2p_offset = g2p_offset\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Replace multiple spaces with one space\n",
    "        # And replace ñ with n\n",
    "        text = re.sub(r'\\s+', ' ', text).replace('ñ', 'n')\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = self.preprocess(text)\n",
    "        parts = re.split(r'({.*?})', text)\n",
    "        result = []\n",
    "        for i, part in enumerate(parts):\n",
    "            if not len(part):\n",
    "                continue\n",
    "            part = part.strip()\n",
    "            if not (part.startswith('{') and part.endswith('}')):\n",
    "                ids = self.tokenizer_eng(part)['input_ids']\n",
    "                result += [i for i in ids]\n",
    "            else:\n",
    "                ids = self.tokenizer_g2p(part[1:-1])['input_ids']\n",
    "                for i,id in enumerate(ids):\n",
    "                    if id in self.special_tokens:\n",
    "                        ids[i] = self.special_tokens[id] - self.g2p_offset\n",
    "                result += [i + self.g2p_offset for i in ids]\n",
    "        return {'input_ids': result, 'attention_mask': list(np.ones_like(result))}\n",
    "\n",
    "    # Returns string constructed from decoded tokens with space handling\n",
    "    def _list_decode(self, input_ids, skip_special_tokens=False):\n",
    "        decode_args = {\n",
    "            'clean_up_tokenization_spaces': True,\n",
    "            'skip_special_tokens': skip_special_tokens\n",
    "        }\n",
    "        output = ''\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                if len(output) == 0 or output[-1] != ' ':\n",
    "                    output += ' '\n",
    "                output += '{'\n",
    "                output += self.tokenizer_g2p.decode(\n",
    "                    [i - self.g2p_offset for i in g],\n",
    "                     **decode_args)\n",
    "                output += '}'\n",
    "            else:\n",
    "                decoded = self.tokenizer_eng.decode(\n",
    "                    g, **decode_args)\n",
    "                if len(output) and output[-1] == '}':\n",
    "                    if len(decoded) and not decoded[0] in string.punctuation:\n",
    "                        output += ' '\n",
    "                output += decoded\n",
    "        return output.strip()\n",
    "\n",
    "    # Returns list of string tokens with no space handling\n",
    "    def _decode_tokens(self, input_ids, skip_special_tokens=False):\n",
    "        toks = []\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                toks.extend(\n",
    "                    [self.tokenizer_g2p.decode(i - self.g2p_offset) for i in g])\n",
    "            else:\n",
    "                toks.extend([self.tokenizer_eng.decode(i) for i in g])\n",
    "        return toks\n",
    "    \n",
    "    def batch_decode(self, input_ids, skip_special_tokens=False):\n",
    "        if not isinstance(input_ids[0], list):\n",
    "            return self._decode_tokens(input_ids)\n",
    "\n",
    "        return [self._list_decode(l, skip_special_tokens) for l in input_ids]\n",
    "\n",
    "prompt_tokenizer = HybridPhonemeTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n",
      "[5, 42, 87, 5, 14, 180, 43]\n",
      "[94, 31, 7, 1, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 2186, 145, 280, 5, 1]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n",
      "[5, 42, 87, 5, 14, 180, 43]\n",
      "[94, 31, 7, 1, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 2186, 145, 280, 5, 1]\n",
      "[5, 42, 87, 5, 14, 180, 43]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "text = \"It's {S OW0 M AH0 CH} larger than life.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode([input_ids], skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "print(prompt_tokenizer(decoded[0])['input_ids'])\n",
    "print(prompt_tokenizer.batch_decode([prompt_tokenizer(decoded[0])['input_ids']], \n",
    "    skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n",
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "prompt_tokenizer2 = HybridPhonemeTokenizer(tokenizer_eng='parler-tts/parler-tts-mini-v1')\n",
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer2(text)['input_ids']\n",
    "decoded = prompt_tokenizer2.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_eng = AutoTokenizer.from_pretrained('parler-tts/parler-tts-mini-v1')\n",
    "tokenizer_eng_new = AutoTokenizer.from_pretrained('synthbot/parlertts_tokenizer_clean')\n",
    "tokenizer_g2p = AutoTokenizer.from_pretrained('tokenizer_g2p_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parler-tts/parler-tts-mini-v1\n",
      "{'input_ids': [2018, 132, 55, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi there!']\n",
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [2018, 132, 55], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "['▁Hi ▁there!']\n",
      "tokenizer_g2p_v2\n",
      "{'input_ids': [40, 21, 40, 5], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "[' H!']\n",
      "hybrid_phoneme_tokenizer\n",
      "{'input_ids': [2018, 132, 55, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi there!']\n",
      "parler-tts/parler-tts-mini-v1\n",
      "{'input_ids': [2645, 9, 132, 55, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Whoa there!']\n",
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [2645, 9, 132, 55], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "['▁Who a ▁there!']\n",
      "tokenizer_g2p_v2\n",
      "{'input_ids': [130, 40, 5], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' W!']\n",
      "hybrid_phoneme_tokenizer\n",
      "{'input_ids': [2645, 9, 132, 55, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Whoa there!']\n",
      "parler-tts/parler-tts-mini-v1\n",
      "{'input_ids': [3, 2, 134, 3, 15251, 632, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['S OW0 ']\n",
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [3, 2, 134, 411, 518, 632, 3, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁ S ▁O W 0 ▁']\n",
      "tokenizer_g2p_v2\n",
      "{'input_ids': [40, 61, 254], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' S OW0 ']\n",
      "hybrid_phoneme_tokenizer\n",
      "[5, 42, 87, 5]\n",
      "{'input_ids': [32105, 32142, 32187, 32105], 'attention_mask': [1, 1, 1, 1]}\n",
      "['{ S OW0 }']\n",
      "parler-tts/parler-tts-mini-v1\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 3, 15251, 632, 283, 3, 14084, 632, 9302, 3, 15251, 632, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"It's S OW0 M AH0 CH OW0\"]\n",
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 411, 518, 632, 283, 71, 566, 632, 205, 566, 411, 518, 632, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"▁It's ▁ S ▁O W 0 ▁M ▁A H 0 ▁C H ▁O W 0\"]\n",
      "tokenizer_g2p_v2\n",
      "{'input_ids': [40, 22, 6, 40, 61, 254, 90, 65, 249, 198], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\" I' S OW0 M AH0 CH OW0\"]\n",
      "hybrid_phoneme_tokenizer\n",
      "[5, 42, 87, 5, 14, 180, 43, 5, 87]\n",
      "{'input_ids': [94, 31, 7, 1, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 32105, 32187], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"It's { S OW0 M AH0 CH OW0}\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [tokenizer_eng, tokenizer_eng_new, tokenizer_g2p, prompt_tokenizer]\n",
    "compare_prompts = [\"Hi there!\", \" Whoa there! \", \"{S OW0 }\", \"It's {S OW0 M AH0 CH OW0}\"]\n",
    "for p in compare_prompts:\n",
    "    for t in tokenizers:\n",
    "        print(t.name_or_path)\n",
    "        tokenized = t(p.strip())\n",
    "        print(tokenized)\n",
    "        print(t.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'need', 'some', 'money', ',', 'can', 'I', 'get', 'some', 'money', '?', 'I', 'need', 'one', 'million', '.', '</s>']\n",
      "['Rai', 's', 'e', 'up', ',', 'get', 'yourself', 'together', ',', 'and', 'drive', 'that', 'fun', 'k', 'y', 'soul', '.', '</s>']\n",
      "['Our', 'results', 'demonstrate', 'high', 'fi', 'de', 'l', 'ity', 'speech', 'generation', 'and', '', 'a', 'diverse', 'range', 'of', 'accent', 's', '</s>']\n",
      "['Equ', 'e', 'stria', 'the', 'brave', 'Equ', 'e', 'stria', 'n', '.', '</s>']\n",
      "['Dead', '', 'n', 'i', 'r', 'i', 'k', 'storage', '.', '</s>']\n",
      "['Well', ',', 'one', 'on', 'one', ',', 'let', \"'\", 's', 'clean', 'it', '!', '</s>']\n",
      "['Amazon', 'delivers', 'packages', 'quickly', 'across', 'the', 'United', 'States', '!', '</s>']\n",
      "['Bitcoin', 'and', 'Ethereum', 'are', 'popular', 'crypto', 'cu', 'r', 'r', 'en', 'c', 'ies', '.', '</s>']\n",
      "['Text', '-', 'to', '-', 's', 'pe', 'e', 'ch', 'models', '1', '$200', '</s>']\n",
      "['Que', 'u', 'e', 'que', 'u', 'ing', '</s>']\n",
      "['Twi', 'light', 'Spark', 'l', 'e', 'Pink', 'i', 'e', 'Pie', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'Ra', 'r', 'ity', 'Rainbow', 'Das', 'h', '</s>']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing',\n",
    "    'Twilight Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash'\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    tokenized = prompt_tokenizer(test_text.strip())\n",
    "    #print(tokenized)\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I {NIY1D SAH1M} money, can I get some money? {AY1 NIY1D WAH1N MIH1LYAH0N}. \n",
      "['I', '</s>', '', '', '', '', 'money', ',', 'can', 'I', 'get', 'some', 'money', '?', '</s>', '', '', '', '', '', '', '', '', '.', '</s>']\n",
      "['I { NIY1D SAH1M} money, can I get some money? { AY1 NIY1D WAH1N MIH1LYAH0N}.']\n",
      "Raise up, get yourself {TAH0GEH1DHER0}, and drive that {FAH1NGKIY0 SOW1L}. \n",
      "['Rai', 's', 'e', 'up', ',', 'get', 'yourself', '</s>', '', '', '', '', '', '', '', ',', 'and', 'drive', 'that', '</s>', '', '', '', '', '', '', '', '.', '</s>']\n",
      "['Raise up, get yourself { TAH0GEH1DHER0}, and drive that { FAH1NGKIY0 SOW1L}.']\n",
      "Our results demonstrate high fidelity {SPIY1CH} generation and a diverse {REY1NJH AH1V} accents \n",
      "['Our', 'results', 'demonstrate', 'high', 'fi', 'de', 'l', 'ity', '</s>', '', '', '', 'generation', 'and', '', 'a', 'diverse', '</s>', '', '', '', '', '', '', 'accent', 's', '</s>']\n",
      "['Our results demonstrate high fidelity { SPIY1CH} generation and a diverse { REY1NJH AH1V} accents']\n",
      "Equestria the {BREY1V IH0KWEH1STRIY0AH0N}. \n",
      "['Equ', 'e', 'stria', 'the', '</s>', '', '', '', '', '', '', '', '', '', '', '', '.', '</s>']\n",
      "['Equestria the { BREY1V IH0KWEH1STRIY0AH0N}.']\n",
      "Dead nirik storage. \n",
      "['Dead', '', 'n', 'i', 'r', 'i', 'k', 'storage', '.', '</s>']\n",
      "['Dead nirik storage.']\n",
      "Well, one on one {,} let's {KLIY1N IH1T}! \n",
      "['Well', ',', 'one', 'on', 'one', '</s>', '', '', 'let', \"'\", 's', '</s>', '', '', '', '', '', '', '!', '</s>']\n",
      "[\"Well, one on one {,} let's { KLIY1N IH1T}!\"]\n",
      "Amazon {DIH0LIH1VER0Z PAE1KAH0JHAH0Z} quickly across the United States! \n",
      "['Amazon', '</s>', '', '', '', '', '', '', '', '', '', 'quickly', 'across', 'the', 'United', 'States', '!', '</s>']\n",
      "['Amazon { DIH0LIH1VER0Z PAE1KAH0JHAH0Z} quickly across the United States!']\n",
      "Bitcoin and Ethereum are popular {KRIH1PKAH0YER2AH0SNIY0Z}. \n",
      "['Bitcoin', 'and', 'Ethereum', 'are', 'popular', '</s>', '', '', '', '', '', '', '', '', '', '', '', '', '.', '</s>']\n",
      "['Bitcoin and Ethereum are popular { KRIH1PKAH0YER2AH0SNIY0Z}.']\n",
      "Text to {SPIY1CH MAA1DAH0LZ} 1 200 \n",
      "['Text', 'to', '</s>', '', '', '', '', '', '', '', '', '1', '200', '</s>']\n",
      "['Text to { SPIY1CH MAA1DAH0LZ} 1 200']\n",
      "Queue queuing \n",
      "['Que', 'u', 'e', 'que', 'u', 'ing', '</s>']\n",
      "['Queue queuing']\n",
      "Twilight {SPAA1RKAH0L PIH1NGKIY0} Pie Fluttershy Applejack Rarity Rainbow Dash \n",
      "['Twi', 'light', '</s>', '', '', '', '', '', '', '', '', 'Pie', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'Ra', 'r', 'ity', 'Rainbow', 'Das', 'h', '</s>']\n",
      "['Twilight { SPAA1RKAH0L PIH1NGKIY0} Pie Fluttershy Applejack Rarity Rainbow Dash']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import groupby\n",
    "from g2p_en import G2p\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "def phonemize(text):\n",
    "    \"\"\"Uses g2p_en to convert a string into contiguous ARPAbet characters\"\"\"\n",
    "    spl = text.split()\n",
    "    l = ''\n",
    "    for s in spl:\n",
    "        p = [arp for arp in g2p(s) if arp != ' ']\n",
    "        arpabet_string = ''.join(p)\n",
    "        l += arpabet_string + ' '\n",
    "    return l.strip()\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"Remove spaces before punctuation.\"\"\"\n",
    "    return re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "\n",
    "def random_phonemize(text, prob=0.2, grow_prob=0.5, seed=0):\n",
    "    \"\"\" Randomly phonemize spans of text.\n",
    "    `prob` influences the base probability of an index being phonemized\n",
    "    `grow_prob` adds a probability for the previous index being phonemized.\"\"\"\n",
    "    text = clean_spaces(text)\n",
    "    # Split including words or isolated punctuation\n",
    "    spl = re.findall(r'[\\w\\']+|[.,!?;:]', text)\n",
    "    splbits = [0 for s in spl]\n",
    "    idxs = list(t[0] for t in enumerate(spl))\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    for idx in idxs[:int(prob*len(spl))]:\n",
    "        splbits[idx] = 1\n",
    "        if random.random() < grow_prob:\n",
    "            if idx > 0:\n",
    "                splbits[idx-1] = 1\n",
    "\n",
    "    ret = ''\n",
    "\n",
    "    for key, group in groupby(enumerate(splbits),\n",
    "        key = lambda t: t[1] == 1):\n",
    "        g = list(group)\n",
    "        g = [spl[t[0]] for t in g]\n",
    "        str_to_process = clean_spaces(' '.join(g))\n",
    "        if key == 0:\n",
    "            ret += str_to_process+' '\n",
    "        else:\n",
    "            ret += '{'+phonemize(str_to_process)+'} '\n",
    "\n",
    "    return clean_spaces(ret)\n",
    "\n",
    "for test_text in test_texts:\n",
    "    #print(random_phonemize(test_text))\n",
    "    rp = random_phonemize(test_text, seed=int(time.time()))\n",
    "    print(rp)\n",
    "    tokenized = prompt_tokenizer(rp.strip())\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "    print(prompt_tokenizer.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
