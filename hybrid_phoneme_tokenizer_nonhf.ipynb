{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "import random\n",
    "from huggingface_hub import hf_hub_download\n",
    "from itertools import groupby\n",
    "from g2p_en import G2p\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"Remove spaces before punctuation and on inside of opening brace.\"\"\"\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'{\\s+','{', text)\n",
    "    return text\n",
    "\n",
    "# Credit to Synthbot for pruned tokenizers\n",
    "class HybridPhonemeTokenizer:\n",
    "    def __init__(self,\n",
    "        tokenizer_eng = 'therealvul/parlertts_tokenizer_clean',\n",
    "        tokenizer_g2p = 'therealvul/g2pen_tokenizer_clean',\n",
    "        eng_special = {\n",
    "            'pad_token': \"<pad>\",\n",
    "            'eos_token': \"</s>\",\n",
    "            'unk_token': \"<unk>\",\n",
    "        },\n",
    "        g2p_special = {\n",
    "            'unk_token': \"[UNK]\",\n",
    "            'pad_token': \"[PAD]\",\n",
    "            'cls_token': \"[CLS]\",\n",
    "            'eos_token': \"[SEP]\",\n",
    "            'mask_token':\"[MASk]\",\n",
    "        },\n",
    "         **kwargs):\n",
    "        self.name_or_path = 'hybrid_phoneme_tokenizer'\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_eng, **eng_special)\n",
    "        self.tokenizer_g2p = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_g2p, **g2p_special)\n",
    "        tokenizer_eng_vocab_path = hf_hub_download(repo_id=\n",
    "            tokenizer_eng, filename=\"tokenizer.json\")\n",
    "        with open(tokenizer_eng_vocab_path, encoding='utf-8') as f:\n",
    "            tokenizer_eng_scores = json.load(f)[\"model\"][\"vocab\"]\n",
    "\n",
    "        # To avoid expanding vocab size and changing embedding length,\n",
    "        # we re-map g2p IDs to disabled IDs in the english tokenizer\n",
    "\n",
    "        # maps from g2p id to external id\n",
    "        self.g2p_to_ext = list()\n",
    "        # maps from external id to g2p id\n",
    "        self.ext_to_g2p = dict()\n",
    "        for i,t in enumerate(tokenizer_eng_scores):\n",
    "            token, score = t\n",
    "            if score == -99.0:\n",
    "                self.g2p_to_ext.append(i)\n",
    "                self.ext_to_g2p[i] = (len(\n",
    "                    self.g2p_to_ext) - 1)\n",
    "\n",
    "        # The vocab size of the g2p tokenizer must be smaller or equal to\n",
    "        # the number of disabled tokens in the eng tokenizer\n",
    "        assert len(self.tokenizer_g2p.get_vocab()) < len(self.g2p_to_ext)\n",
    "\n",
    "        # Not sure if this is actually necessary - ByteLevel pretokenizer\n",
    "        # removes possibility of <unk> tokens\n",
    "        self.special_tokens = {\n",
    "            self.tokenizer_g2p.pad_token_id: self.tokenizer_eng.pad_token_id,\n",
    "            self.tokenizer_g2p.bos_token_id: self.tokenizer_eng.bos_token_id,\n",
    "            self.tokenizer_g2p.cls_token_id: self.tokenizer_eng.cls_token_id,\n",
    "            self.tokenizer_g2p.eos_token_id: self.tokenizer_eng.eos_token_id,\n",
    "            self.tokenizer_g2p.unk_token_id: self.tokenizer_eng.unk_token_id,\n",
    "            self.tokenizer_g2p.mask_token_id: self.tokenizer_eng.mask_token_id\n",
    "        }\n",
    "        self.pad_token_id = self.tokenizer_eng.pad_token_id\n",
    "\n",
    "    def ext_is_g2p_id(self, id):\n",
    "        return id in self.ext_to_g2p\n",
    "\n",
    "    def ext_to_g2p_id(self, id):\n",
    "        return self.ext_to_g2p[id]\n",
    "\n",
    "    def g2p_to_ext_id(self, id):\n",
    "        return self.g2p_to_ext[id]\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Replace multiple spaces with one space\n",
    "        # And replace ñ with n\n",
    "        text = re.sub(r'\\s+', ' ', text).replace('ñ', 'n')\n",
    "        return text\n",
    "\n",
    "    def max_vocab_length(self):\n",
    "        return len(self.tokenizer_eng.get_vocab())\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = self.preprocess(text)\n",
    "        parts = re.split(r'({.*?})', text)\n",
    "        result = []\n",
    "        for i, part in enumerate(parts):\n",
    "            if not len(part):\n",
    "                continue\n",
    "            part = part.strip()\n",
    "            if not (part.startswith('{') and part.endswith('}')):\n",
    "                ids = self.tokenizer_eng(part, add_special_tokens=False)['input_ids']\n",
    "                result += [i for i in ids]\n",
    "            else:\n",
    "                ids = self.tokenizer_g2p(part[1:-1])['input_ids']\n",
    "                for i,id in enumerate(ids):\n",
    "                    if id in self.special_tokens:\n",
    "                        ids[i] = self.special_tokens[id]\n",
    "                    else:\n",
    "                        ids[i] = self.g2p_to_ext_id(id)\n",
    "                result += [i for i in ids]\n",
    "        return {'input_ids': result, 'attention_mask': list(np.ones_like(result))}\n",
    "\n",
    "    # Returns string constructed from decoded tokens with space handling\n",
    "    def _list_decode(self, input_ids, skip_special_tokens=False):\n",
    "        decode_args = {\n",
    "            'clean_up_tokenization_spaces': True,\n",
    "            'skip_special_tokens': skip_special_tokens\n",
    "        }\n",
    "        output = ''\n",
    "        for isg2p, group in groupby(input_ids,\n",
    "            key=lambda x: self.ext_is_g2p_id(x)):\n",
    "            g = list(group)\n",
    "            if isg2p:\n",
    "                if len(output) == 0 or output[-1] != ' ':\n",
    "                    output += ' '\n",
    "                output += '{'\n",
    "                output += self.tokenizer_g2p.decode(\n",
    "                    [self.ext_to_g2p_id(i) for i in g],\n",
    "                     **decode_args)\n",
    "                output += '}'\n",
    "            else:\n",
    "                decoded = self.tokenizer_eng.decode(\n",
    "                    g, **decode_args)\n",
    "                if len(output) and output[-1] == '}':\n",
    "                    if len(decoded) and not decoded[0] in string.punctuation:\n",
    "                        output += ' '\n",
    "                output += decoded\n",
    "        return clean_spaces(output.strip())\n",
    "\n",
    "    # Returns list of string tokens with no space handling\n",
    "    def _decode_tokens(self, input_ids, skip_special_tokens=False):\n",
    "        toks = []\n",
    "        for isg2p, group in groupby(input_ids,\n",
    "            key=lambda x: self.ext_is_g2p_id(x)):\n",
    "            g = list(group)\n",
    "            if isg2p:\n",
    "                toks.extend(\n",
    "                    [self.tokenizer_g2p.decode(\n",
    "                        self.ext_to_g2p_id(i)) for i in g])\n",
    "            else:\n",
    "                toks.extend([self.tokenizer_eng.decode(i) for i in g])\n",
    "        return toks\n",
    "    \n",
    "    def batch_decode(self, input_ids, skip_special_tokens=False):\n",
    "        if not isinstance(input_ids[0], list):\n",
    "            return self._decode_tokens(input_ids)\n",
    "\n",
    "        return [self._list_decode(l, skip_special_tokens) for l in input_ids]\n",
    "\n",
    "    \n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "class HorsePhonemizer:\n",
    "    def __init__(self, horsewords_dictionary = 'new_horsewords.clean'):\n",
    "        self.horsedict = {}\n",
    "        with open(horsewords_dictionary, 'r') as f:\n",
    "            while line := f.readline():\n",
    "                baseword, transcription = line.split('  ')\n",
    "                self.horsedict[baseword] = transcription\n",
    "\n",
    "    def phonemize(self, text):\n",
    "        \"\"\"Uses g2p_en + a dictionary to convert a string into contiguous ARPAbet characters\"\"\"\n",
    "        spl = text.split()\n",
    "        l = ''\n",
    "        for s in spl:\n",
    "            s_up = s.strip().upper()\n",
    "            if s_up in self.horsedict:\n",
    "                arpabet = ''.join(self.horsedict[s_up].split())\n",
    "                l += arpabet + ' '\n",
    "            else:\n",
    "                p = [arp for arp in g2p(s) if arp != ' ']\n",
    "                arpabet_string = ''.join(p)\n",
    "                l += arpabet_string + ' '\n",
    "        return l.strip()\n",
    "\n",
    "    def random_phonemize(self, text, prob=0.2, grow_prob=0.2, seed=None):\n",
    "        \"\"\" Randomly phonemize spans of text.\n",
    "        `prob` influences the base probability of an index being phonemized\n",
    "        `grow_prob` adds a probability for the previous index being phonemized.\"\"\"\n",
    "        text = clean_spaces(text)\n",
    "        # Split including words or isolated punctuation\n",
    "        spl = re.findall(r'[\\w\\']+|[.,!?;:]', text)\n",
    "        splbits = [0 for s in spl]\n",
    "        idxs = list(t[0] for t in enumerate(spl))\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "        for idx in idxs[:int(prob*len(spl))]:\n",
    "            splbits[idx] = 1\n",
    "            if random.random() < grow_prob:\n",
    "                if idx > 0:\n",
    "                    splbits[idx-1] = 1\n",
    "\n",
    "        ret = ''\n",
    "\n",
    "        for key, group in groupby(enumerate(splbits),\n",
    "            key = lambda t: t[1] == 1):\n",
    "            g = list(group)\n",
    "            g = [spl[t[0]] for t in g]\n",
    "            str_to_process = clean_spaces(' '.join(g))\n",
    "            if key == 0:\n",
    "                ret += str_to_process+' '\n",
    "            else:\n",
    "                ret += '{'+self.phonemize(str_to_process)+'} '\n",
    "\n",
    "        return clean_spaces(ret)\n",
    "\n",
    "prompt_tokenizer = HybridPhonemeTokenizer()\n",
    "print(prompt_tokenizer.max_vocab_length())\n",
    "\n",
    "class HorsePhonemizer:\n",
    "    def __init__(self, horsewords_dictionary = 'new_horsewords.clean'):\n",
    "        self.horsedict = {}\n",
    "        with open(horsewords_dictionary, 'r') as f:\n",
    "            while line := f.readline():\n",
    "                baseword, transcription = line.split('  ')\n",
    "                self.horsedict[baseword] = transcription\n",
    "\n",
    "    def phonemize(self, text):\n",
    "        \"\"\"Uses g2p_en + a dictionary to convert a string into contiguous ARPAbet characters\"\"\"\n",
    "        spl = text.split()\n",
    "        l = ''\n",
    "        for s in spl:\n",
    "            s_up = s.strip().upper()\n",
    "            if s_up in self.horsedict:\n",
    "                arpabet = ''.join(self.horsedict[s_up].split())\n",
    "                l += arpabet + ' '\n",
    "            else:\n",
    "                p = [arp for arp in g2p(s) if arp != ' ']\n",
    "                arpabet_string = ''.join(p)\n",
    "                l += arpabet_string + ' '\n",
    "        return l.strip()\n",
    "\n",
    "    def random_phonemize(self, text, prob=0.2, grow_prob=0.2, seed=None):\n",
    "        \"\"\" Randomly phonemize spans of text.\n",
    "        `prob` influences the base probability of an index being phonemized\n",
    "        `grow_prob` adds a probability for the previous index being phonemized.\"\"\"\n",
    "        text = clean_spaces(text)\n",
    "        # Split including words or isolated punctuation\n",
    "        spl = re.findall(r'[\\w\\']+|[.,!?;:]', text)\n",
    "        splbits = [0 for s in spl]\n",
    "        idxs = list(t[0] for t in enumerate(spl))\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "        for idx in idxs[:int(prob*len(spl))]:\n",
    "            splbits[idx] = 1\n",
    "            if random.random() < grow_prob:\n",
    "                if idx > 0:\n",
    "                    splbits[idx-1] = 1\n",
    "\n",
    "        ret = ''\n",
    "\n",
    "        for key, group in groupby(enumerate(splbits),\n",
    "            key = lambda t: t[1] == 1):\n",
    "            g = list(group)\n",
    "            g = [spl[t[0]] for t in g]\n",
    "            str_to_process = clean_spaces(' '.join(g))\n",
    "            if key == 0:\n",
    "                ret += str_to_process+' '\n",
    "            else:\n",
    "                ret += '{'+self.phonemize(str_to_process)+'} '\n",
    "\n",
    "        return clean_spaces(ret)\n",
    "\n",
    "horse_phonemizer = HorsePhonemizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come on Rainbow Dash {, YUW1 KAE1N} do {DHIH1S}. Just remember the routine!\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Come on Rainbow Dash, you can do this. Just remember the routine!\"\n",
    "ids = prompt_tokenizer(test_text)['input_ids']\n",
    "text = prompt_tokenizer.batch_decode([ids])[0]\n",
    "text = horse_phonemizer.random_phonemize(text)\n",
    "ids = prompt_tokenizer(text)['input_ids']\n",
    "text = prompt_tokenizer.batch_decode([ids])[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 3, 88, 9, 162, 29, 56, 36, 3, 9, 1555, 3, 88, 9, 162, 29, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5]\n",
      "['And', 'my', '', 'he', 'a', 've', 'n', 'will', 'be', '', 'a', 'mare', '', 'he', 'a', 've', 'n', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.']\n",
      "[94, 31, 7, 93, 252, 374, 93, 171, 583, 256, 50, 52, 122, 49, 145, 280, 5]\n",
      "[\"It's {S OW0 M AH0 CH} larger than life.\"]\n",
      "[94, 31, 7, 93, 252, 374, 93, 171, 583, 256, 50, 52, 122, 49, 145, 280, 5]\n",
      "[\"It's {S OW0 M AH0 CH} larger than life.\"]\n",
      "[False, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "text = \"It's {S OW0 M AH0 CH} larger than life.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode([input_ids], skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "print(prompt_tokenizer(decoded[0])['input_ids'])\n",
    "print(prompt_tokenizer.batch_decode([prompt_tokenizer(decoded[0])['input_ids']], \n",
    "    skip_special_tokens=True))\n",
    "\n",
    "print([prompt_tokenizer.ext_is_g2p_id(i) for i in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 3, 88, 9, 162, 29, 56, 36, 3, 9, 1555, 3, 88, 9, 162, 29, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5]\n",
      "['And', 'my', '', 'he', 'a', 've', 'n', 'will', 'be', '', 'a', 'mare', '', 'he', 'a', 've', 'n', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "#prompt_tokenizer2 = HybridPhonemeTokenizer(tokenizer_eng='parler-tts/parler-tts-mini-v1')\n",
    "#text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "#input_ids = prompt_tokenizer2(text)['input_ids']\n",
    "#decoded = prompt_tokenizer2.batch_decode(input_ids, skip_special_tokens=True)\n",
    "#print(input_ids)\n",
    "#print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_eng = AutoTokenizer.from_pretrained('parler-tts/parler-tts-mini-v1')\n",
    "tokenizer_eng_new = AutoTokenizer.from_pretrained('synthbot/parlertts_tokenizer_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parler-tts/parler-tts-mini-v1\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 3, 15251, 632, 283, 3, 14084, 632, 9302, 3, 15251, 632, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', '', '', 'S', '', 'OW', '0', 'M', '', 'AH', '0', 'CH', '', 'OW', '0', '', '']\n",
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 411, 518, 632, 283, 71, 566, 632, 9302, 411, 518, 632, 2, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', '', '', 'S', 'O', 'W', '0', 'M', 'A', 'H', '0', 'CH', 'O', 'W', '0', '', '']\n",
      "hybrid_phoneme_tokenizer\n",
      "{'input_ids': [94, 31, 7, 93, 252, 374, 93, 171, 583, 256, 93, 374], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', ' ', 'S ', 'OW0', ' ', 'M', ' AH0 ', 'CH', ' ', 'OW0']\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [#tokenizer_eng, tokenizer_eng_new, tokenizer_g2p,\n",
    "tokenizer_eng,\n",
    " tokenizer_eng_new,\n",
    " prompt_tokenizer,\n",
    " ]\n",
    "compare_prompts = [\"It's {S OW0 M AH0 CH OW0}\"]\n",
    "for p in compare_prompts:\n",
    "    for t in tokenizers:\n",
    "        print(t.name_or_path)\n",
    "        tokenized = t(p.strip())\n",
    "        print(tokenized)\n",
    "        print(t.batch_decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'need', 'some', '', 'mon', 'e', 'y', ',', 'can', 'I', 'get', 'some', '', 'mon', 'e', 'y', '?', 'I', 'need', 'one', '', 'm', 'i', 'll', 'i', 'on', '.']\n",
      "['R', 'a', 'is', 'e', 'up', ',', 'get', 'yourself', 'together', ',', 'and', '', 'd', 'r', 'ive', 'that', 'fun', 'k', 'y', 'so', 'ul', '.']\n",
      "['Our', '', 're', 's', 'ul', 't', 's', 'de', 'mon', 's', 't', 'r', 'at', 'e', 'high', '', 'f', 'i', 'de', 'l', 'ity', '', 's', 'pe', 'e', 'ch', '', 'g', 'en', 'er', 'ation', 'and', '', 'a', '', 'd', 'ive', 'r', 's', 'e', '', 'r', 'an', 'g', 'e', 'of', '', 'a', 'c', 'c', 'en', 't', 's']\n",
      "['E', 'que', 'stria', 'the', '', 'b', 'r', 'a', 've', 'E', 'que', 'stria', 'n', '.']\n",
      "['', 'D', 'e', 'a', 'd', '', 'n', 'i', 'r', 'i', 'k', '', 's', 't', 'or', 'a', 'g', 'e', '.']\n",
      "['Well', ',', 'one', 'on', 'one', ',', 'let', \"'\", 's', 'clean', 'it', '!']\n",
      "['A', 'm', 'a', 'z', 'on', 'de', 'l', 'ive', 'r', 's', '', 'p', 'a', 'c', 'k', 'a', 'g', 'e', 's', 'quick', 'ly', '', 'a', 'c', 'r', 'o', 's', 's', 'the', 'U', 'n', 'it', 'e', 'd', 'S', 't', 'at', 'e', 's', '!']\n",
      "['', 'B', 'it', 'c', 'o', 'in', 'and', 'E', 't', 'he', 're', 'u', 'm', 'are', 'po', 'p', 'ul', 'ar', '', 'c', 'ry', 'p', 'to', 'cu', 'r', 'r', 'en', 'c', 'ies', '.']\n",
      "['T', 'e', 'x', 't', '-', 'to', '-', 's', 'pe', 'e', 'ch', '', 'm', 'o', 'de', 'l', 's', '', '1', '', '$', '2', '0', '0']\n",
      "['', 'Q', 'u', 'e', 'u', 'e', '', 'que', 'u', 'ing']\n",
      "['Twi', 'light', 'Sp', 'ar', 'k', 'le', 'Pink', 'i', 'e', 'P', 'i', 'e', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'R', 'ar', 'ity', 'Rainbow', 'Das', 'h', 'Ah', 'u', 'i', 'z', 'o', 't', 'l', 'T', 'en', 'o', 'ch', 't', 'it', 'l', 'an', '', 'X', 'il', 'at', 'i', '', 'Z', 'i', 'e', 'g', 'f', 'il', 'ly']\n",
      "['R', 'e', 'p', 'ar', 'at', 'u', 'r']\n",
      "['S', 'er', 'v', 'i', 'c', 'i', 'i']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing',\n",
    "    'Twilight Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash Ahuizotl Tenochtitlan Xilati Ziegfilly',\n",
    "    'Reparatur',\n",
    "    'Servicii',\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    tokenized = prompt_tokenizer(test_text.strip())\n",
    "    #print(tokenized)\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need some money {, KAE1N} I get some money? I need one million {.} \n",
      "['I', 'need', 'some', '', 'mon', 'e', 'y', ' ', ',', ' ', 'K', 'AE1N', 'I', 'get', 'some', '', 'mon', 'e', 'y', '?', 'I', 'need', 'one', '', 'm', 'i', 'll', 'i', 'on', ' ', '.']\n",
      "['I need some money {, KAE1N} I get some money? I need one million {.}']\n",
      "Raise {AH1P}, get yourself together, and {DRAY1V} that funky soul. \n",
      "['R', 'a', 'is', 'e', ' ', 'AH1', 'P', '', ',', 'get', 'yourself', 'together', ',', 'and', ' ', 'DR', 'AY1', 'V', 'that', 'fun', 'k', 'y', 'so', 'ul', '.']\n",
      "['Raise {AH1P}, get yourself together, and {DRAY1V} that funky soul.']\n",
      "Our {RIH0ZAH1LTS} demonstrate high fidelity speech generation and {AH0} diverse range of accents \n",
      "['Our', ' ', 'RIH0', 'Z', 'AH1L', 'T', 'S', 'de', 'mon', 's', 't', 'r', 'at', 'e', 'high', '', 'f', 'i', 'de', 'l', 'ity', '', 's', 'pe', 'e', 'ch', '', 'g', 'en', 'er', 'ation', 'and', ' ', 'AH0', '', 'd', 'ive', 'r', 's', 'e', '', 'r', 'an', 'g', 'e', 'of', '', 'a', 'c', 'c', 'en', 't', 's']\n",
      "['Our {RIH0ZAH1LTS} demonstrate high fidelity speech generation and {AH0} diverse range of accents']\n",
      "{IY0KWEH0STRIY0AH0 DHAH0} brave Equestrian. \n",
      "[' ', 'IY0', 'KW', 'EH0', 'ST', 'RIY0', 'AH0 ', 'DH', 'AH0', '', 'b', 'r', 'a', 've', 'E', 'que', 'stria', 'n', '.']\n",
      "['{IY0KWEH0STRIY0AH0 DHAH0} brave Equestrian.']\n",
      "Dead nirik storage. \n",
      "['', 'D', 'e', 'a', 'd', '', 'n', 'i', 'r', 'i', 'k', '', 's', 't', 'or', 'a', 'g', 'e', '.']\n",
      "['Dead nirik storage.']\n",
      "Well, {WAH1N} on one, {LEH1TS} clean it! \n",
      "['Well', ',', ' ', 'W', 'AH1N', 'on', 'one', ',', ' ', 'LEH1', 'T', 'S', 'clean', 'it', '!']\n",
      "['Well, {WAH1N} on one, {LEH1TS} clean it!']\n",
      "Amazon delivers {PAE1KAH0JHAH0Z} quickly across the United States! \n",
      "['A', 'm', 'a', 'z', 'on', 'de', 'l', 'ive', 'r', 's', ' ', 'P', 'AE1K', 'AH0JH', 'AH0Z', 'quick', 'ly', '', 'a', 'c', 'r', 'o', 's', 's', 'the', 'U', 'n', 'it', 'e', 'd', 'S', 't', 'at', 'e', 's', '!']\n",
      "['Amazon delivers {PAE1KAH0JHAH0Z} quickly across the United States!']\n",
      "Bitcoin and Ethereum {AA1R} popular cryptocurrencies. \n",
      "['', 'B', 'it', 'c', 'o', 'in', 'and', 'E', 't', 'he', 're', 'u', 'm', ' ', 'AA1R', 'po', 'p', 'ul', 'ar', '', 'c', 'ry', 'p', 'to', 'cu', 'r', 'r', 'en', 'c', 'ies', '.']\n",
      "['Bitcoin and Ethereum {AA1R} popular cryptocurrencies.']\n",
      "{TEH1KST TUW1} speech models 1 200 \n",
      "[' ', 'T', 'EH1KS', 'T ', 'TUW1', '', 's', 'pe', 'e', 'ch', '', 'm', 'o', 'de', 'l', 's', '', '1', '', '2', '0', '0']\n",
      "['{TEH1KST TUW1} speech models 1 200']\n",
      "Queue queuing \n",
      "['', 'Q', 'u', 'e', 'u', 'e', '', 'que', 'u', 'ing']\n",
      "['Queue queuing']\n",
      "Twilight {SPAA1RKAH0L} Pinkie Pie Fluttershy Applejack Rarity Rainbow {DAE1SH} Ahuizotl Tenochtitlan Xilati Ziegfilly \n",
      "['Twi', 'light', ' ', 'SP', 'AA1R', 'KAH0L', 'Pink', 'i', 'e', 'P', 'i', 'e', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'R', 'ar', 'ity', 'Rainbow', ' ', 'D', 'AE1', 'SH', 'Ah', 'u', 'i', 'z', 'o', 't', 'l', 'T', 'en', 'o', 'ch', 't', 'it', 'l', 'an', '', 'X', 'il', 'at', 'i', '', 'Z', 'i', 'e', 'g', 'f', 'il', 'ly']\n",
      "['Twilight {SPAA1RKAH0L} Pinkie Pie Fluttershy Applejack Rarity Rainbow {DAE1SH} Ahuizotl Tenochtitlan Xilati Ziegfilly']\n",
      "Reparatur \n",
      "['R', 'e', 'p', 'ar', 'at', 'u', 'r']\n",
      "['Reparatur']\n",
      "Servicii \n",
      "['S', 'er', 'v', 'i', 'c', 'i', 'i']\n",
      "['Servicii']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import groupby\n",
    "from g2p_en import G2p\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "class HorsePhonemizer:\n",
    "    def __init__(self, horsewords_dictionary = 'new_horsewords.clean'):\n",
    "        self.horsedict = {}\n",
    "        with open(horsewords_dictionary, 'r') as f:\n",
    "            while line := f.readline():\n",
    "                baseword, transcription = line.split('  ')\n",
    "                self.horsedict[baseword] = transcription\n",
    "\n",
    "    def phonemize(self, text):\n",
    "        \"\"\"Uses g2p_en + a dictionary to convert a string into contiguous ARPAbet characters\"\"\"\n",
    "        spl = text.split()\n",
    "        l = ''\n",
    "        for s in spl:\n",
    "            s_up = s.strip().upper()\n",
    "            if s_up in self.horsedict:\n",
    "                arpabet = ''.join(self.horsedict[s_up].split())\n",
    "                l += arpabet + ' '\n",
    "            else:\n",
    "                p = [arp for arp in g2p(s) if arp != ' ']\n",
    "                arpabet_string = ''.join(p)\n",
    "                l += arpabet_string + ' '\n",
    "        return l.strip()\n",
    "\n",
    "    def random_phonemize(self, text, prob=0.2, grow_prob=0.2, seed=0):\n",
    "        \"\"\" Randomly phonemize spans of text.\n",
    "        `prob` influences the base probability of an index being phonemized\n",
    "        `grow_prob` adds a probability for the previous index being phonemized.\"\"\"\n",
    "        text = clean_spaces(text)\n",
    "        # Split including words or isolated punctuation\n",
    "        spl = re.findall(r'[\\w\\']+|[.,!?;:]', text)\n",
    "        splbits = [0 for s in spl]\n",
    "        idxs = list(t[0] for t in enumerate(spl))\n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "        for idx in idxs[:int(prob*len(spl))]:\n",
    "            splbits[idx] = 1\n",
    "            if random.random() < grow_prob:\n",
    "                if idx > 0:\n",
    "                    splbits[idx-1] = 1\n",
    "\n",
    "        ret = ''\n",
    "\n",
    "        for key, group in groupby(enumerate(splbits),\n",
    "            key = lambda t: t[1] == 1):\n",
    "            g = list(group)\n",
    "            g = [spl[t[0]] for t in g]\n",
    "            str_to_process = clean_spaces(' '.join(g))\n",
    "            if key == 0:\n",
    "                ret += str_to_process+' '\n",
    "            else:\n",
    "                ret += '{'+self.phonemize(str_to_process)+'} '\n",
    "\n",
    "        return clean_spaces(ret)\n",
    "\n",
    "hphzr = HorsePhonemizer()\n",
    "for test_text in test_texts:\n",
    "    #print(random_phonemize(test_text))\n",
    "    rp = hphzr.random_phonemize(test_text, seed=int(time.time()))\n",
    "    tokenized = prompt_tokenizer(rp.strip())\n",
    "    print(rp)\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "    print(prompt_tokenizer.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
