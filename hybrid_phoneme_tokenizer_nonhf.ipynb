{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "class HybridPhonemeTokenizer:\n",
    "    def __init__(self,\n",
    "        tokenizer_eng = 'parler-tts/parler-tts-mini-v1',\n",
    "        tokenizer_g2p = 'therealvul/tokenizer_g2pen_v2',\n",
    "        eng_special = {\n",
    "            'pad_token': \"<pad>\",\n",
    "            'sep_token': \"</s>\",\n",
    "            'unk_token': \"<unk>\",\n",
    "        },\n",
    "        g2p_special = {\n",
    "            'unk_token': \"[UNK]\",\n",
    "            'pad_token': \"[PAD]\",\n",
    "            'cls_token': \"[CLS]\",\n",
    "            'sep_token': \"[SEP]\",\n",
    "            'mask_token':\"[MASk]\",\n",
    "        },\n",
    "         **kwargs):\n",
    "\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_eng, **eng_special)\n",
    "        self.tokenizer_g2p = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_g2p, **g2p_special)\n",
    "\n",
    "        # Not sure if this is actually necessary - ByteLevel pretokenizer\n",
    "        # removes possibility of <unk> tokens\n",
    "        self.special_tokens = {\n",
    "            self.tokenizer_g2p.pad_token_id: self.tokenizer_eng.pad_token_id,\n",
    "            self.tokenizer_g2p.bos_token_id: self.tokenizer_eng.bos_token_id,\n",
    "            self.tokenizer_g2p.cls_token_id: self.tokenizer_eng.cls_token_id,\n",
    "            self.tokenizer_g2p.eos_token_id: self.tokenizer_eng.eos_token_id,\n",
    "            self.tokenizer_g2p.unk_token_id: self.tokenizer_eng.unk_token_id,\n",
    "            self.tokenizer_g2p.mask_token_id: self.tokenizer_eng.mask_token_id\n",
    "        }\n",
    "\n",
    "        self.g2p_offset = self.prune_tokens()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        parts = re.split(r'({.*?})', text)\n",
    "        result = []\n",
    "        for i, part in enumerate(parts):\n",
    "            if not len(part):\n",
    "                continue\n",
    "            part = part.strip()\n",
    "            if not (part.startswith('{') and part.endswith('}')):\n",
    "                ids = self.tokenizer_eng(part)['input_ids']\n",
    "                result += [i for i in ids]\n",
    "            else:\n",
    "                ids = self.tokenizer_g2p(part[1:-1])['input_ids']\n",
    "                print(ids)\n",
    "                for i,id in enumerate(ids):\n",
    "                    if id in self.special_tokens:\n",
    "                        ids[i] = self.special_tokens[id] - self.g2p_offset\n",
    "                result += [i + self.g2p_offset for i in ids]\n",
    "        return {'input_ids': result, 'attention_mask': list(np.ones_like(result))}\n",
    "\n",
    "    def prune_tokens(self):\n",
    "        # Prune the tokenizer here?\n",
    "        g2p_offset = len(self.tokenizer_eng.get_vocab())\n",
    "        return g2p_offset\n",
    "    \n",
    "    # Returns string constructed from decoded tokens with space handling\n",
    "    def _list_decode(self, input_ids, skip_special_tokens=False):\n",
    "        decode_args = {\n",
    "            'clean_up_tokenization_spaces': True,\n",
    "            'skip_special_tokens': skip_special_tokens\n",
    "        }\n",
    "        output = ''\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                if len(output) == 0 or output[-1] != ' ':\n",
    "                    output += ' '\n",
    "                output += '{'\n",
    "                output += self.tokenizer_g2p.decode(\n",
    "                    [i - self.g2p_offset for i in g],\n",
    "                     **decode_args)\n",
    "                output += '}'\n",
    "            else:\n",
    "                decoded = self.tokenizer_eng.decode(\n",
    "                    g, **decode_args)\n",
    "                if len(output) and output[-1] == '}':\n",
    "                    if len(decoded) and not decoded[0] in string.punctuation:\n",
    "                        output += ' '\n",
    "                output += decoded\n",
    "        return output.strip()\n",
    "\n",
    "    # Returns list of string tokens with no space handling\n",
    "    def _decode_tokens(self, input_ids, skip_special_tokens=False):\n",
    "        toks = []\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                toks.extend([self.tokenizer_g2p.decode(i) for i in g])\n",
    "            else:\n",
    "                toks.extend([self.tokenizer_eng.decode(i) for i in g])\n",
    "        return toks\n",
    "    \n",
    "    def batch_decode(self, input_ids, skip_special_tokens=False):\n",
    "        if not isinstance(input_ids[0], list):\n",
    "            return self._decode_tokens(input_ids)\n",
    "\n",
    "        return [self._list_decode(l, skip_special_tokens) for l in input_ids]\n",
    "\n",
    "prompt_tokenizer = HybridPhonemeTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 9922, 56, 36, 3, 9, 600, 9922, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'big', 'heaven', '.', '</s>']\n",
      "[40, 61, 254, 90, 65, 100]\n",
      "[94, 31, 7, 1, 32140, 32161, 32354, 32190, 32165, 32200, 2186, 145, 280, 5, 1]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n",
      "[40, 61, 254, 90, 65, 100]\n",
      "[94, 31, 7, 1, 32140, 32161, 32354, 32190, 32165, 32200, 2186, 145, 280, 5, 1]\n",
      "[40, 61, 254, 90, 65, 100]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a big heaven.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "text = \"It's {S OW0 M AH0 CH} larger than life.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode([input_ids], skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "print(prompt_tokenizer(decoded[0])['input_ids'])\n",
    "print(prompt_tokenizer.batch_decode([prompt_tokenizer(decoded[0])['input_ids']], \n",
    "    skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_eng = AutoTokenizer.from_pretrained('parler-tts/parler-tts-mini-v1')\n",
    "tokenizer_g2p = AutoTokenizer.from_pretrained('tokenizer_g2p_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2018, 132, 55, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi there!']\n",
      "{'input_ids': [40, 21, 40, 5], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "[' H!']\n",
      "{'input_ids': [2018, 132, 55, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi there!']\n",
      "{'input_ids': [2645, 9, 132, 55, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Whoa there!']\n",
      "{'input_ids': [130, 40, 5], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' W!']\n",
      "{'input_ids': [2645, 9, 132, 55, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Whoa there!']\n",
      "{'input_ids': [3, 2, 134, 3, 15251, 632, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['S OW0 ']\n",
      "{'input_ids': [40, 61, 254], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' S OW0 ']\n",
      "[40, 61, 254]\n",
      "{'input_ids': [32140, 32161, 32354], 'attention_mask': [1, 1, 1]}\n",
      "['{ S OW0 }']\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 3, 15251, 632, 283, 3, 14084, 632, 9302, 3, 15251, 632, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"It's S OW0 M AH0 CH OW0\"]\n",
      "{'input_ids': [40, 22, 6, 40, 61, 254, 90, 65, 249, 198], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\" I' S OW0 M AH0 CH OW0\"]\n",
      "[40, 61, 254, 90, 65, 249, 198]\n",
      "{'input_ids': [94, 31, 7, 1, 32140, 32161, 32354, 32190, 32165, 32349, 32298], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"It's { S OW0 M AH0 CH OW0}\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [tokenizer_eng, tokenizer_g2p, prompt_tokenizer]\n",
    "compare_prompts = [\"Hi there!\", \" Whoa there! \", \"{S OW0 }\", \"It's {S OW0 M AH0 CH OW0}\"]\n",
    "for p in compare_prompts:\n",
    "    for t in tokenizers:\n",
    "        tokenized = t(p.strip())\n",
    "        print(tokenized)\n",
    "        print(t.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
