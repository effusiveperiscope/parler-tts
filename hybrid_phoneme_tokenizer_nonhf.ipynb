{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"Remove spaces before punctuation and on inside of opening brace.\"\"\"\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'{\\s+','{', text)\n",
    "    return text\n",
    "\n",
    "# Credit to Synthbot for pruned tokenizers\n",
    "class HybridPhonemeTokenizer:\n",
    "    def __init__(self,\n",
    "        tokenizer_eng = 'synthbot/parlertts_tokenizer_clean',\n",
    "        tokenizer_g2p = 'synthbot/vul_g2pen_tokenizer_clean',\n",
    "        eng_special = {\n",
    "            'pad_token': \"<pad>\",\n",
    "            'eos_token': \"</s>\",\n",
    "            'unk_token': \"<unk>\",\n",
    "        },\n",
    "        g2p_special = {\n",
    "            'unk_token': \"[UNK]\",\n",
    "            'pad_token': \"[PAD]\",\n",
    "            'cls_token': \"[CLS]\",\n",
    "            'eos_token': \"[SEP]\",\n",
    "            'mask_token':\"[MASk]\",\n",
    "        },\n",
    "         **kwargs):\n",
    "        self.name_or_path = 'hybrid_phoneme_tokenizer'\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_eng, **eng_special)\n",
    "        self.tokenizer_g2p = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_g2p, **g2p_special)\n",
    "\n",
    "        # Not sure if this is actually necessary - ByteLevel pretokenizer\n",
    "        # removes possibility of <unk> tokens\n",
    "        self.special_tokens = {\n",
    "            self.tokenizer_g2p.pad_token_id: self.tokenizer_eng.pad_token_id,\n",
    "            self.tokenizer_g2p.bos_token_id: self.tokenizer_eng.bos_token_id,\n",
    "            self.tokenizer_g2p.cls_token_id: self.tokenizer_eng.cls_token_id,\n",
    "            self.tokenizer_g2p.eos_token_id: self.tokenizer_eng.eos_token_id,\n",
    "            self.tokenizer_g2p.unk_token_id: self.tokenizer_eng.unk_token_id,\n",
    "            self.tokenizer_g2p.mask_token_id: self.tokenizer_eng.mask_token_id\n",
    "        }\n",
    "\n",
    "        g2p_offset = len(self.tokenizer_eng.get_vocab())\n",
    "        self.g2p_offset = g2p_offset\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Replace multiple spaces with one space\n",
    "        # And replace ñ with n\n",
    "        text = re.sub(r'\\s+', ' ', text).replace('ñ', 'n')\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = self.preprocess(text)\n",
    "        parts = re.split(r'({.*?})', text)\n",
    "        result = []\n",
    "        for i, part in enumerate(parts):\n",
    "            if not len(part):\n",
    "                continue\n",
    "            part = part.strip()\n",
    "            if not (part.startswith('{') and part.endswith('}')):\n",
    "                ids = self.tokenizer_eng(part, add_special_tokens=False)['input_ids']\n",
    "                result += [i for i in ids]\n",
    "            else:\n",
    "                ids = self.tokenizer_g2p(part[1:-1])['input_ids']\n",
    "                for i,id in enumerate(ids):\n",
    "                    if id in self.special_tokens:\n",
    "                        ids[i] = self.special_tokens[id] - self.g2p_offset\n",
    "                result += [i + self.g2p_offset for i in ids]\n",
    "        return {'input_ids': result, 'attention_mask': list(np.ones_like(result))}\n",
    "\n",
    "    # Returns string constructed from decoded tokens with space handling\n",
    "    def _list_decode(self, input_ids, skip_special_tokens=False):\n",
    "        decode_args = {\n",
    "            'clean_up_tokenization_spaces': True,\n",
    "            'skip_special_tokens': skip_special_tokens\n",
    "        }\n",
    "        output = ''\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                if len(output) == 0 or output[-1] != ' ':\n",
    "                    output += ' '\n",
    "                output += '{'\n",
    "                output += self.tokenizer_g2p.decode(\n",
    "                    [i - self.g2p_offset for i in g],\n",
    "                     **decode_args)\n",
    "                output += '}'\n",
    "            else:\n",
    "                decoded = self.tokenizer_eng.decode(\n",
    "                    g, **decode_args)\n",
    "                if len(output) and output[-1] == '}':\n",
    "                    if len(decoded) and not decoded[0] in string.punctuation:\n",
    "                        output += ' '\n",
    "                output += decoded\n",
    "        return clean_spaces(output.strip())\n",
    "\n",
    "    # Returns list of string tokens with no space handling\n",
    "    def _decode_tokens(self, input_ids, skip_special_tokens=False):\n",
    "        toks = []\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            g = list(group)\n",
    "            if key:\n",
    "                toks.extend(\n",
    "                    [self.tokenizer_g2p.decode(i - self.g2p_offset) for i in g])\n",
    "            else:\n",
    "                toks.extend([self.tokenizer_eng.decode(i) for i in g])\n",
    "        return toks\n",
    "    \n",
    "    def batch_decode(self, input_ids, skip_special_tokens=False):\n",
    "        if not isinstance(input_ids[0], list):\n",
    "            return self._decode_tokens(input_ids)\n",
    "\n",
    "        return [self._list_decode(l, skip_special_tokens) for l in input_ids]\n",
    "\n",
    "prompt_tokenizer = HybridPhonemeTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n",
      "[94, 31, 7, 1, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 2186, 145, 280, 5, 1]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n",
      "[94, 31, 7, 1, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 2186, 145, 280, 5, 1]\n",
      "[\"It's { S OW0 M AH0 CH} larger than life.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "text = \"It's {S OW0 M AH0 CH} larger than life.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode([input_ids], skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "print(prompt_tokenizer(decoded[0])['input_ids'])\n",
    "print(prompt_tokenizer.batch_decode([prompt_tokenizer(decoded[0])['input_ids']], \n",
    "    skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n",
      "[275, 82, 9922, 56, 36, 3, 9, 1555, 9922, 6, 11, 27, 56, 1482, 190, 8, 851, 1365, 5, 1]\n",
      "['And', 'my', 'heaven', 'will', 'be', '', 'a', 'mare', 'heaven', ',', 'and', 'I', 'will', 'walk', 'through', 'the', 'front', 'door', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "\n",
    "prompt_tokenizer2 = HybridPhonemeTokenizer(tokenizer_eng='parler-tts/parler-tts-mini-v1')\n",
    "text = \"And my heaven will be a mare heaven, and I will walk through the front door.\"\n",
    "input_ids = prompt_tokenizer2(text)['input_ids']\n",
    "decoded = prompt_tokenizer2.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_eng = AutoTokenizer.from_pretrained('parler-tts/parler-tts-mini-v1')\n",
    "tokenizer_eng_new = AutoTokenizer.from_pretrained('synthbot/parlertts_tokenizer_clean')\n",
    "tokenizer_g2p = AutoTokenizer.from_pretrained('tokenizer_g2p_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthbot/parlertts_tokenizer_clean\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 411, 518, 632, 283, 71, 566, 632, 9302, 411, 518, 632, 2, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', '', '', 'S', 'O', 'W', '0', 'M', 'A', 'H', '0', 'CH', 'O', 'W', '0', '', '']\n",
      "hybrid_phoneme_tokenizer\n",
      "{'input_ids': [94, 31, 7, 32105, 32142, 32187, 32105, 32114, 32280, 32143, 32105, 32187], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', ' ', 'S ', 'OW0', ' ', 'M', ' AH0 ', 'CH', ' ', 'OW0']\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [#tokenizer_eng, tokenizer_eng_new, tokenizer_g2p,\n",
    "#tokenizer_eng\n",
    " tokenizer_eng_new,\n",
    " prompt_tokenizer,\n",
    " ]\n",
    "compare_prompts = [\"It's {S OW0 M AH0 CH OW0}\"]\n",
    "for p in compare_prompts:\n",
    "    for t in tokenizers:\n",
    "        print(t.name_or_path)\n",
    "        tokenized = t(p.strip())\n",
    "        print(tokenized)\n",
    "        print(t.batch_decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'need', 'some', 'money', ',', 'can', 'I', 'get', 'some', 'money', '?', 'I', 'need', 'one', 'million', '.']\n",
      "['Rai', 's', 'e', 'up', ',', 'get', 'yourself', 'together', ',', 'and', 'drive', 'that', 'fun', 'k', 'y', 'soul', '.']\n",
      "['Our', 'results', 'demonstrate', 'high', 'fi', 'de', 'l', 'ity', 'speech', 'generation', 'and', '', 'a', 'diverse', 'range', 'of', 'accent', 's']\n",
      "['Equ', 'e', 'stria', 'the', 'brave', 'Equ', 'e', 'stria', 'n', '.']\n",
      "['Dead', '', 'n', 'i', 'r', 'i', 'k', 'storage', '.']\n",
      "['Well', ',', 'one', 'on', 'one', ',', 'let', \"'\", 's', 'clean', 'it', '!']\n",
      "['Amazon', 'delivers', 'packages', 'quickly', 'across', 'the', 'United', 'States', '!']\n",
      "['Bitcoin', 'and', 'Ethereum', 'are', 'popular', 'crypto', 'cu', 'r', 'r', 'en', 'c', 'ies', '.']\n",
      "['Text', '-', 'to', '-', 's', 'pe', 'e', 'ch', 'models', '1', '$200']\n",
      "['Que', 'u', 'e', 'que', 'u', 'ing']\n",
      "['Twi', 'light', 'Spark', 'l', 'e', 'Pink', 'i', 'e', 'Pie', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'Ra', 'r', 'ity', 'Rainbow', 'Das', 'h']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "  'I need some money, can I get some money? I need one million.',\n",
    "    'Raise up, get yourself together, and drive that funky soul.',\n",
    "    'Our results demonstrate high fidelity speech generation and a diverse range of accents',\n",
    "    'Equestria the brave Equestrian.',\n",
    "    'Dead nirik storage.',\n",
    "    'Well, one on one, let\\'s clean it!',\n",
    "    'Amazon delivers packages quickly across the United States!',\n",
    "    'Bitcoin and Ethereum are popular cryptocurrencies.',\n",
    "    'Text-to-speech models 1 $200',\n",
    "    'Queue queuing',\n",
    "    'Twilight Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash'\n",
    "    ]\n",
    "for test_text in test_texts:\n",
    "    tokenized = prompt_tokenizer(test_text.strip())\n",
    "    #print(tokenized)\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{AY1} need some money {,} can {AY1} get some money? I need one million. \n",
      "[' ', 'AY1', 'need', 'some', 'money', ' ', ',', 'can', ' ', 'AY1', 'get', 'some', 'money', '?', 'I', 'need', 'one', 'million', '.']\n",
      "['{AY1} need some money {,} can {AY1} get some money? I need one million.']\n",
      "{REY1Z} up, get yourself {TAH0GEH1DHER0,} and drive that funky soul. \n",
      "[' ', 'REY1', 'Z', 'up', ',', 'get', 'yourself', ' ', 'TAH0', 'G', 'EH1', 'DH', 'ER0', ',', 'and', 'drive', 'that', 'fun', 'k', 'y', 'soul', '.']\n",
      "['{REY1Z} up, get yourself {TAH0GEH1DHER0,} and drive that funky soul.']\n",
      "{AW1ER0} results demonstrate high fidelity {SPIY1CH JHEH2NER0EY1SHAH0N} and a diverse range of accents \n",
      "[' ', 'AW1', 'ER0', 'results', 'demonstrate', 'high', 'fi', 'de', 'l', 'ity', ' ', 'SP', 'IY1CH ', 'JH', 'EH2N', 'ER0', 'EY1SHAH0N', 'and', '', 'a', 'diverse', 'range', 'of', 'accent', 's']\n",
      "['{AW1ER0} results demonstrate high fidelity {SPIY1CH JHEH2NER0EY1SHAH0N} and a diverse range of accents']\n",
      "Equestria the {BREY1V} Equestrian. \n",
      "['Equ', 'e', 'stria', 'the', ' ', 'B', 'REY1', 'V', 'Equ', 'e', 'stria', 'n', '.']\n",
      "['Equestria the {BREY1V} Equestrian.']\n",
      "Dead nirik storage. \n",
      "['Dead', '', 'n', 'i', 'r', 'i', 'k', 'storage', '.']\n",
      "['Dead nirik storage.']\n",
      "{WEH1L}, one on one {, LEH1TS} clean it! \n",
      "[' ', 'WEH1L', '', ',', 'one', 'on', 'one', ' ', ',', ' ', 'LEH1', 'T', 'S', 'clean', 'it', '!']\n",
      "['{WEH1L}, one on one {, LEH1TS} clean it!']\n",
      "{AE1MAH0ZAA2N} delivers packages quickly across the United States! \n",
      "[' ', 'AE1M', 'AH0Z', 'AA2N', 'delivers', 'packages', 'quickly', 'across', 'the', 'United', 'States', '!']\n",
      "['{AE1MAH0ZAA2N} delivers packages quickly across the United States!']\n",
      "Bitcoin and {EH1THRIY0AH0M} are popular cryptocurrencies. \n",
      "['Bitcoin', 'and', ' ', 'EH1', 'TH', 'RIY0', 'AH0M', 'are', 'popular', 'crypto', 'cu', 'r', 'r', 'en', 'c', 'ies', '.']\n",
      "['Bitcoin and {EH1THRIY0AH0M} are popular cryptocurrencies.']\n",
      "Text to {SPIY1CH} models 1 200 \n",
      "['Text', 'to', ' S', 'PIY1', 'CH', 'models', '1', '200']\n",
      "['Text to {SPIY1CH} models 1 200']\n",
      "Queue queuing \n",
      "['Que', 'u', 'e', 'que', 'u', 'ing']\n",
      "['Queue queuing']\n",
      "{TWAY1LAY2T} Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash \n",
      "[' ', 'T', 'WAY1', 'L', 'AY2', 'T', 'Spark', 'l', 'e', 'Pink', 'i', 'e', 'Pie', 'F', 'lutter', 's', 'h', 'y', 'Apple', 'jack', 'Ra', 'r', 'ity', 'Rainbow', 'Das', 'h']\n",
      "['{TWAY1LAY2T} Sparkle Pinkie Pie Fluttershy Applejack Rarity Rainbow Dash']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import groupby\n",
    "from g2p_en import G2p\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "def phonemize(text):\n",
    "    \"\"\"Uses g2p_en to convert a string into contiguous ARPAbet characters\"\"\"\n",
    "    spl = text.split()\n",
    "    l = ''\n",
    "    for s in spl:\n",
    "        p = [arp for arp in g2p(s) if arp != ' ']\n",
    "        arpabet_string = ''.join(p)\n",
    "        l += arpabet_string + ' '\n",
    "    return l.strip()\n",
    "\n",
    "def random_phonemize(text, prob=0.2, grow_prob=0.5, seed=0):\n",
    "    \"\"\" Randomly phonemize spans of text.\n",
    "    `prob` influences the base probability of an index being phonemized\n",
    "    `grow_prob` adds a probability for the previous index being phonemized.\"\"\"\n",
    "    text = clean_spaces(text)\n",
    "    # Split including words or isolated punctuation\n",
    "    spl = re.findall(r'[\\w\\']+|[.,!?;:]', text)\n",
    "    splbits = [0 for s in spl]\n",
    "    idxs = list(t[0] for t in enumerate(spl))\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    for idx in idxs[:int(prob*len(spl))]:\n",
    "        splbits[idx] = 1\n",
    "        if random.random() < grow_prob:\n",
    "            if idx > 0:\n",
    "                splbits[idx-1] = 1\n",
    "\n",
    "    ret = ''\n",
    "\n",
    "    for key, group in groupby(enumerate(splbits),\n",
    "        key = lambda t: t[1] == 1):\n",
    "        g = list(group)\n",
    "        g = [spl[t[0]] for t in g]\n",
    "        str_to_process = clean_spaces(' '.join(g))\n",
    "        if key == 0:\n",
    "            ret += str_to_process+' '\n",
    "        else:\n",
    "            ret += '{'+phonemize(str_to_process)+'} '\n",
    "\n",
    "    return clean_spaces(ret)\n",
    "\n",
    "for test_text in test_texts:\n",
    "    #print(random_phonemize(test_text))\n",
    "    rp = random_phonemize(test_text, seed=int(time.time()))\n",
    "    tokenized = prompt_tokenizer(rp.strip())\n",
    "    print(rp)\n",
    "    print(prompt_tokenizer.batch_decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "    print(prompt_tokenizer.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
