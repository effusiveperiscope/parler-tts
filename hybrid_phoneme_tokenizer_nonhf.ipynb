{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class HybridPhonemeTokenizer:\n",
    "    def __init__(self,\n",
    "        tokenizer_eng = 'parler-tts/parler-tts-mini-v1',\n",
    "        tokenizer_g2p = 'therealvul/tokenizer_g2pen_v2',\n",
    "        eng_prefix = 'eng:',\n",
    "        g2p_prefix = 'g2p:',\n",
    "        # maps eng tokens to g2p\n",
    "        # TODO special tokens\n",
    "        special_tokens = {\n",
    "            \"<pad>\": \"[PAD]\",\n",
    "            \"</s>\": \"[SEP]\",\n",
    "            \"<unk>\": \"[UNK]\",\n",
    "            \"<extra_id_99>\": \"[CLS]\",\n",
    "            \"<extra_id_98>\": \"[MASK]\",},\n",
    "         **kwargs):\n",
    "\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_eng)\n",
    "        self.tokenizer_g2p = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_g2p)\n",
    "        self.eng_prefix = eng_prefix\n",
    "        self.g2p_prefix = g2p_prefix\n",
    "        self.special_tokens = special_tokens\n",
    "        self.g2p_offset = self.prune_tokens()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        parts = re.split(r'({.*?})', text)\n",
    "        result = []\n",
    "        for i, part in enumerate(parts):\n",
    "            if not len(part):\n",
    "                continue\n",
    "            part = part.strip()\n",
    "            if not (part.startswith('{') and part.endswith('}')):\n",
    "                ids = self.tokenizer_eng(part)['input_ids']\n",
    "                result += [i for i in ids]\n",
    "            else:\n",
    "                ids = self.tokenizer_g2p(part[1:-1])['input_ids']\n",
    "                result += [i + self.g2p_offset for i in ids]\n",
    "        return {'input_ids': result, 'attention_mask': list(np.ones_like(result))}\n",
    "\n",
    "    def prune_tokens(self):\n",
    "        # Prune the tokenizer here?\n",
    "        g2p_offset = len(self.tokenizer_eng.get_vocab())\n",
    "        return g2p_offset\n",
    "    \n",
    "    def _list_decode(self, input_ids, skip_special_tokens=False):\n",
    "        decode_args = {\n",
    "            'clean_up_tokenization_spaces': True,\n",
    "            'skip_special_tokens': skip_special_tokens\n",
    "        }\n",
    "        output = ''\n",
    "        for key, group in groupby(input_ids,\n",
    "            key=lambda x: x >= self.g2p_offset):\n",
    "            if key:\n",
    "                if len(output) == 0 or output[-1] != ' ':\n",
    "                    output += ' '\n",
    "                output += '{'\n",
    "                output += self.tokenizer_g2p.decode(\n",
    "                    [i - self.g2p_offset for i in list(group)],\n",
    "                     **decode_args)\n",
    "                output += '} '\n",
    "            else:\n",
    "                output += self.tokenizer_eng.decode(\n",
    "                    list(group), **decode_args)\n",
    "        return output.strip()\n",
    "    \n",
    "    def batch_decode(self, input_ids, skip_special_tokens=False):\n",
    "        if not isinstance(input_ids[0], list):\n",
    "            input_ids = [input_ids] # TODO Not correct\n",
    "\n",
    "        return [self._list_decode(l, skip_special_tokens) for l in input_ids]\n",
    "\n",
    "prompt_tokenizer = HybridPhonemeTokenizer(\n",
    "    tokenizer_eng='meta-llama/Meta-Llama-3.1-8B'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 2181, 596, 128296, 128317, 128510, 128346, 128321, 128356, 128000, 26, 1102, 596, 264, 128404, 128296, 128304, 128645, 128296, 128311, 128270, 128292, 128315, 128282]\n",
      "[\"It's { S OW0 M AH0 CH} ; It's a { B IH0 G T AY0 M}\"]\n",
      "[128000, 2181, 596, 128296, 128317, 128510, 128346, 128321, 128356, 128000, 26, 1102, 596, 264, 128404, 128296, 128304, 128645, 128296, 128311, 128270, 128292, 128315, 128282]\n",
      "[\"It's { S OW0 M AH0 CH} ; It's a { B IH0 G T AY0 M}\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"It's {S OW0 M AH0 CH}; It's a {B IH0 G T AY0 M}\"\n",
    "input_ids = prompt_tokenizer(text)['input_ids']\n",
    "decoded = prompt_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(input_ids)\n",
    "print(decoded)\n",
    "print(prompt_tokenizer(decoded[0])['input_ids'])\n",
    "print(prompt_tokenizer.batch_decode(prompt_tokenizer(decoded[0])['input_ids'], \n",
    "    skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_eng = AutoTokenizer.from_pretrained('parler-tts/parler-tts-mini-v1')\n",
    "tokenizer_g2p = AutoTokenizer.from_pretrained('tokenizer_g2p_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2018, 132, 55, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi', 'there', '!', '']\n",
      "{'input_ids': [40, 21, 40, 5], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "[' ', 'H', ' ', '!']\n",
      "{'input_ids': [128000, 13347, 1070, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "['Hi there!']\n",
      "{'input_ids': [2645, 9, 132, 55, 1], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Who', 'a', 'there', '!', '']\n",
      "{'input_ids': [130, 40, 5], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' W', ' ', '!']\n",
      "{'input_ids': [128000, 15546, 64, 1070, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['Whoa there!']\n",
      "{'input_ids': [3, 2, 134, 3, 15251, 632, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['', '', 'S', '', 'OW', '0', '', '', '']\n",
      "{'input_ids': [40, 61, 254], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "[' ', 'S ', 'OW0 ']\n",
      "{'input_ids': [128296, 128317, 128510], 'attention_mask': [1, 1, 1]}\n",
      "['{ S OW0 }']\n",
      "{'input_ids': [94, 31, 7, 3, 2, 134, 3, 15251, 632, 283, 3, 14084, 632, 9302, 3, 15251, 632, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['It', \"'\", 's', '', '', 'S', '', 'OW', '0', 'M', '', 'AH', '0', 'CH', '', 'OW', '0', '', '']\n",
      "{'input_ids': [40, 22, 6, 40, 61, 254, 90, 65, 249, 198], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[' ', 'I', \"'\", ' ', 'S ', 'OW0 ', 'M ', 'AH0 ', 'CH ', 'OW0']\n",
      "{'input_ids': [128000, 2181, 596, 128296, 128317, 128510, 128346, 128321, 128505, 128454], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[\"It's { S OW0 M AH0 CH OW0}\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [tokenizer_eng, tokenizer_g2p, prompt_tokenizer]\n",
    "compare_prompts = [\"Hi there!\", \" Whoa there! \", \"{S OW0 }\", \"It's {S OW0 M AH0 CH OW0}\"]\n",
    "for p in compare_prompts:\n",
    "    for t in tokenizers:\n",
    "        tokenized = t(p.strip())\n",
    "        print(tokenized)\n",
    "        print(t.batch_decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "        #print(t.batch_decode([tokenized['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
